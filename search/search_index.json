{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Lupyne is a search engine based on PyLucene, the Python extension for accessing Java Lucene.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code>&gt;&gt;&gt; from lupyne import engine                       # don't forget to call lucene.initVM\n&gt;&gt;&gt; indexer = engine.Indexer('temp')                # create an index at path\n&gt;&gt;&gt; indexer.set('name', stored=True)                # create stored 'name' field\n&gt;&gt;&gt; indexer.set('text', engine.Field.Text)          # create indexed 'text' field\n&gt;&gt;&gt; indexer.add(name='sample', text='hello world')  # add a document to the index\n&gt;&gt;&gt; indexer.commit()                                # commit changes; document is now searchable\n&gt;&gt;&gt; hits = indexer.search('text:hello')             # run search and return sequence of documents\n&gt;&gt;&gt; len(hits), hits.count                           # 1 hit retrieved (out of a total of 1)\n(1, 1)\n&gt;&gt;&gt; (hit,) = hits\n&gt;&gt;&gt; hit['name']                                     # hits support mapping interface for their stored fields\n'sample'\n&gt;&gt;&gt; hit.id, hit.score                               # plus internal doc number and score\n(0, 0.28768208622932434)\n&gt;&gt;&gt; hit.dict()                                      # dict representation of the hit document\n{'name': 'sample', '__id__': 0, '__score__': 0.28768208622932434}\n</code></pre>"},{"location":"engine/","title":"Engine","text":""},{"location":"engine/#analyzers","title":"analyzers","text":""},{"location":"engine/#lupyne.engine.analyzers.TokenStream","title":"<code>lupyne.engine.analyzers.TokenStream</code>","text":"<p>               Bases: <code>TokenStream</code></p> <p>TokenStream mixin with support for iteration and attributes cached as properties.</p> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>class TokenStream(analysis.TokenStream):\n    \"\"\"TokenStream mixin with support for iteration and attributes cached as properties.\"\"\"\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.incrementToken():\n            return self\n        raise StopIteration\n\n    def __getattr__(self, name):\n        cls = getattr(analysis.tokenattributes, name + 'Attribute').class_\n        attr = self.getAttribute(cls) if self.hasAttribute(cls) else self.addAttribute(cls)\n        setattr(self, name, attr)\n        return attr\n\n    @property\n    def offset(self) -&gt; tuple:\n        \"\"\"start and stop character offset\"\"\"\n        return self.Offset.startOffset(), self.Offset.endOffset()\n\n    @offset.setter\n    def offset(self, item: Iterable):\n        self.Offset.setOffset(*item)\n\n    @property\n    def payload(self):\n        \"\"\"payload bytes\"\"\"\n        payload = self.Payload.payload\n        return payload and payload.utf8ToString()\n\n    @payload.setter\n    def payload(self, data):\n        self.Payload.payload = util.BytesRef(data)\n\n    @property\n    def positionIncrement(self) -&gt; int:\n        \"\"\"position relative to the previous token\"\"\"\n        return self.PositionIncrement.positionIncrement\n\n    @positionIncrement.setter\n    def positionIncrement(self, index: int):\n        self.PositionIncrement.positionIncrement = index\n\n    @property\n    def charTerm(self) -&gt; str:\n        \"\"\"term text\"\"\"\n        return str(self.CharTerm)\n\n    @charTerm.setter\n    def charTerm(self, text: str):\n        self.CharTerm.setEmpty()\n        self.CharTerm.append(text)\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"lexical type\"\"\"\n        return self.Type.type()\n\n    @type.setter\n    def type(self, text: str):\n        self.Type.setType(text)\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.TokenStream.charTerm","title":"<code>charTerm</code>  <code>property</code> <code>writable</code>","text":"<p>term text</p>"},{"location":"engine/#lupyne.engine.analyzers.TokenStream.offset","title":"<code>offset</code>  <code>property</code> <code>writable</code>","text":"<p>start and stop character offset</p>"},{"location":"engine/#lupyne.engine.analyzers.TokenStream.payload","title":"<code>payload</code>  <code>property</code> <code>writable</code>","text":"<p>payload bytes</p>"},{"location":"engine/#lupyne.engine.analyzers.TokenStream.positionIncrement","title":"<code>positionIncrement</code>  <code>property</code> <code>writable</code>","text":"<p>position relative to the previous token</p>"},{"location":"engine/#lupyne.engine.analyzers.TokenStream.type","title":"<code>type</code>  <code>property</code> <code>writable</code>","text":"<p>lexical type</p>"},{"location":"engine/#lupyne.engine.analyzers.TokenFilter","title":"<code>lupyne.engine.analyzers.TokenFilter</code>","text":"<p>               Bases: <code>PythonTokenFilter</code>, <code>TokenStream</code></p> <p>Create an iterable lucene TokenFilter from a TokenStream.</p> <p>Subclass and override incrementToken.</p> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>class TokenFilter(PythonTokenFilter, TokenStream):\n    \"\"\"Create an iterable lucene TokenFilter from a TokenStream.\n\n    Subclass and override [incrementToken][lupyne.engine.analyzers.TokenFilter.incrementToken].\n    \"\"\"\n\n    def __init__(self, input: analysis.TokenStream):\n        super().__init__(input)\n        self.input = input\n        self.reset()\n\n    def incrementToken(self) -&gt; bool:\n        \"\"\"Advance to next token and return whether the stream is not empty.\"\"\"\n        return self.input.incrementToken()\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.TokenFilter.incrementToken","title":"<code>incrementToken()</code>","text":"<p>Advance to next token and return whether the stream is not empty.</p> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>def incrementToken(self) -&gt; bool:\n    \"\"\"Advance to next token and return whether the stream is not empty.\"\"\"\n    return self.input.incrementToken()\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.Analyzer","title":"<code>lupyne.engine.analyzers.Analyzer</code>","text":"<p>               Bases: <code>PythonAnalyzer</code></p> <p>Return a lucene Analyzer which chains together a tokenizer and filters.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Callable</code> <p>lucene Tokenizer class or callable, called with no args</p> required <code>*filters</code> <code>Callable</code> <p>lucene TokenFilter classes or callables, successively called on input tokens</p> <code>()</code> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>class Analyzer(PythonAnalyzer):\n    \"\"\"Return a lucene Analyzer which chains together a tokenizer and filters.\n\n    Args:\n        tokenizer: lucene Tokenizer class or callable, called with no args\n        *filters: lucene TokenFilter classes or callables, successively called on input tokens\n    \"\"\"\n\n    def __init__(self, tokenizer: Callable, *filters: Callable):\n        super().__init__()\n        self.tokenizer, self.filters = tokenizer, filters\n\n    @classmethod\n    def standard(cls, *filters: Callable) -&gt; 'Analyzer':\n        \"\"\"Return equivalent of StandardAnalyzer with additional filters.\"\"\"\n        return cls(analysis.standard.StandardTokenizer, analysis.LowerCaseFilter, *filters)\n\n    @classmethod\n    def whitespace(cls, *filters: Callable) -&gt; 'Analyzer':\n        \"\"\"Return equivalent of WhitespaceAnalyzer with additional filters.\"\"\"\n        return cls(analysis.core.WhitespaceTokenizer, *filters)\n\n    def components(self, field, reader=None):\n        source = tokens = self.tokenizer()\n        if reader is not None:\n            source.reader = reader\n        for filter in self.filters:\n            tokens = filter(tokens)\n        return source, tokens\n\n    def createComponents(self, field):\n        return analysis.Analyzer.TokenStreamComponents(*self.components(field))\n\n    def tokens(self, text: str, field: str | None = None) -&gt; analysis.TokenStream:\n        \"\"\"Return lucene TokenStream from text.\"\"\"\n        return self.components(field, StringReader(text))[1]\n\n    def parse(self, query: str, field='', op='', parser=None, **attrs) -&gt; search.Query:\n        \"\"\"Return parsed lucene Query.\n\n        Args:\n            query: query string\n            field: default query field name, sequence of names, or boost mapping\n            op: default query operator ('or', 'and')\n            parser: custom PythonQueryParser class\n            **attrs: additional attributes to set on the parser\n        \"\"\"\n        # parsers aren't thread-safe (nor slow), so create one each time\n        cls = queryparser.classic.MultiFieldQueryParser\n        if isinstance(field, str):\n            cls = queryparser.classic.QueryParser\n        args: tuple = field, self\n        if isinstance(field, Mapping):\n            boosts = HashMap()\n            for key in field:\n                boosts.put(key, Float(field[key]))\n            args = list(field), self, boosts\n        parser = (parser or cls)(*args)\n        if op:\n            parser.defaultOperator = getattr(queryparser.classic.QueryParser.Operator, op.upper())\n        for name, value in attrs.items():\n            setattr(parser, name, value)\n        if isinstance(parser, queryparser.classic.MultiFieldQueryParser):\n            return parser.parse(parser, query)\n        return parser.parse(query)\n\n    def highlight(self, query: search.Query, field: str, content: str, count: int = 1) -&gt; str:\n        \"\"\"Return highlighted content.\n\n        Args:\n            query: lucene Query\n            field: field name\n            content: text\n            count: optional maximum number of passages\n        \"\"\"\n        highlighter = uhighlight.UnifiedHighlighter(None, self)\n        return str(highlighter.highlightWithoutSearcher(field, query, content, count))\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.Analyzer.highlight","title":"<code>highlight(query, field, content, count=1)</code>","text":"<p>Return highlighted content.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Query</code> <p>lucene Query</p> required <code>field</code> <code>str</code> <p>field name</p> required <code>content</code> <code>str</code> <p>text</p> required <code>count</code> <code>int</code> <p>optional maximum number of passages</p> <code>1</code> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>def highlight(self, query: search.Query, field: str, content: str, count: int = 1) -&gt; str:\n    \"\"\"Return highlighted content.\n\n    Args:\n        query: lucene Query\n        field: field name\n        content: text\n        count: optional maximum number of passages\n    \"\"\"\n    highlighter = uhighlight.UnifiedHighlighter(None, self)\n    return str(highlighter.highlightWithoutSearcher(field, query, content, count))\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.Analyzer.parse","title":"<code>parse(query, field='', op='', parser=None, **attrs)</code>","text":"<p>Return parsed lucene Query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <code>field</code> <p>default query field name, sequence of names, or boost mapping</p> <code>''</code> <code>op</code> <p>default query operator ('or', 'and')</p> <code>''</code> <code>parser</code> <p>custom PythonQueryParser class</p> <code>None</code> <code>**attrs</code> <p>additional attributes to set on the parser</p> <code>{}</code> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>def parse(self, query: str, field='', op='', parser=None, **attrs) -&gt; search.Query:\n    \"\"\"Return parsed lucene Query.\n\n    Args:\n        query: query string\n        field: default query field name, sequence of names, or boost mapping\n        op: default query operator ('or', 'and')\n        parser: custom PythonQueryParser class\n        **attrs: additional attributes to set on the parser\n    \"\"\"\n    # parsers aren't thread-safe (nor slow), so create one each time\n    cls = queryparser.classic.MultiFieldQueryParser\n    if isinstance(field, str):\n        cls = queryparser.classic.QueryParser\n    args: tuple = field, self\n    if isinstance(field, Mapping):\n        boosts = HashMap()\n        for key in field:\n            boosts.put(key, Float(field[key]))\n        args = list(field), self, boosts\n    parser = (parser or cls)(*args)\n    if op:\n        parser.defaultOperator = getattr(queryparser.classic.QueryParser.Operator, op.upper())\n    for name, value in attrs.items():\n        setattr(parser, name, value)\n    if isinstance(parser, queryparser.classic.MultiFieldQueryParser):\n        return parser.parse(parser, query)\n    return parser.parse(query)\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.Analyzer.standard","title":"<code>standard(*filters)</code>  <code>classmethod</code>","text":"<p>Return equivalent of StandardAnalyzer with additional filters.</p> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>@classmethod\ndef standard(cls, *filters: Callable) -&gt; 'Analyzer':\n    \"\"\"Return equivalent of StandardAnalyzer with additional filters.\"\"\"\n    return cls(analysis.standard.StandardTokenizer, analysis.LowerCaseFilter, *filters)\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.Analyzer.tokens","title":"<code>tokens(text, field=None)</code>","text":"<p>Return lucene TokenStream from text.</p> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>def tokens(self, text: str, field: str | None = None) -&gt; analysis.TokenStream:\n    \"\"\"Return lucene TokenStream from text.\"\"\"\n    return self.components(field, StringReader(text))[1]\n</code></pre>"},{"location":"engine/#lupyne.engine.analyzers.Analyzer.whitespace","title":"<code>whitespace(*filters)</code>  <code>classmethod</code>","text":"<p>Return equivalent of WhitespaceAnalyzer with additional filters.</p> Source code in <code>lupyne/engine/analyzers.py</code> <pre><code>@classmethod\ndef whitespace(cls, *filters: Callable) -&gt; 'Analyzer':\n    \"\"\"Return equivalent of WhitespaceAnalyzer with additional filters.\"\"\"\n    return cls(analysis.core.WhitespaceTokenizer, *filters)\n</code></pre>"},{"location":"engine/#indexers","title":"indexers","text":""},{"location":"engine/#lupyne.engine.indexers.IndexReader","title":"<code>lupyne.engine.indexers.IndexReader</code>","text":"<p>Delegated lucene IndexReader, with a mapping interface of ids to document objects.</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <p>lucene IndexReader</p> required Source code in <code>lupyne/engine/indexers.py</code> <pre><code>class IndexReader:\n    \"\"\"Delegated lucene IndexReader, with a mapping interface of ids to document objects.\n\n    Args:\n        reader: lucene IndexReader\n    \"\"\"\n\n    def __init__(self, reader):\n        self.indexReader = reader\n\n    def __getattr__(self, name):\n        if name == 'indexReader':\n            raise AttributeError(name)\n        return getattr(index.DirectoryReader.cast_(self.indexReader), name)\n\n    def __len__(self):\n        return self.numDocs()\n\n    def __contains__(self, id: int):\n        bits = self.bits\n        return (0 &lt;= id &lt; self.maxDoc()) and (not bits or bits.get(id))\n\n    def __iter__(self) -&gt; Iterator[int]:\n        ids = range(self.maxDoc())\n        bits = self.bits\n        return filter(bits.get, ids) if bits else iter(ids)\n\n    @property\n    def bits(self) -&gt; util.Bits:\n        return index.MultiBits.getLiveDocs(self.indexReader)\n\n    @property\n    def directory(self) -&gt; store.Directory:\n        \"\"\"reader's lucene Directory\"\"\"\n        return self.__getattr__('directory')()\n\n    @property\n    def path(self) -&gt; str:\n        \"\"\"FSDirectory path\"\"\"\n        return str(store.FSDirectory.cast_(self.directory).directory)\n\n    @property\n    def timestamp(self) -&gt; float:\n        \"\"\"timestamp of reader's last commit\"\"\"\n        return File(self.path, self.indexCommit.segmentsFileName).lastModified() * 0.001\n\n    @property\n    def readers(self) -&gt; Iterator:\n        \"\"\"segment readers\"\"\"\n        return (index.SegmentReader.cast_(context.reader()) for context in self.leaves())\n\n    @property\n    def segments(self) -&gt; dict:\n        \"\"\"segment filenames with document counts\"\"\"\n        return {reader.segmentName: reader.numDocs() for reader in self.readers}\n\n    @property\n    def fieldinfos(self) -&gt; dict:\n        \"\"\"mapping of field names to lucene FieldInfos\"\"\"\n        fieldinfos = index.FieldInfos.getMergedFieldInfos(self.indexReader)\n        return {fieldinfo.name: fieldinfo for fieldinfo in fieldinfos.iterator()}\n\n    def dictionary(self, name: str, *args) -&gt; spell.Dictionary:\n        \"\"\"Return lucene Dictionary, suitable for spellcheckers.\"\"\"\n        cls = spell.HighFrequencyDictionary if args else spell.LuceneDictionary\n        return cls(self.indexReader, name, *args)\n\n    def suggest(self, name: str, value, count: int = 1, **attrs) -&gt; list:\n        \"\"\"Return spelling suggestions from DirectSpellChecker.\n\n        Args:\n            name: field name\n            value: term\n            count: maximum number of suggestions\n            **attrs: DirectSpellChecker options\n        \"\"\"\n        checker = spell.DirectSpellChecker()\n        for attr in attrs:\n            setattr(checker, attr, attrs[attr])\n        words = checker.suggestSimilar(index.Term(name, value), count, self.indexReader)\n        return [word.string for word in words]\n\n    def complete(self, name: str, prefix: str, count: int) -&gt; list[str]:\n        \"\"\"Return autocomplete suggestions for word prefix.\"\"\"\n        terms = dict(self.terms(name, prefix, counts=True))\n        return heapq.nlargest(count, terms, key=terms.__getitem__)\n\n    def sortfield(self, name: str, type=None, reverse=False) -&gt; search.SortField:\n        \"\"\"Return lucene SortField, deriving the the type from FieldInfos if necessary.\n\n        Args:\n            name: field name\n            type: int, float, or name compatible with SortField constants\n            reverse: reverse flag used with sort\n        \"\"\"\n        if type is None:\n            type = str(self.fieldinfos[name].docValuesType)\n        type = Field.types.get(type, type).upper()\n        return search.SortField(name, getattr(search.SortField.Type, type), reverse)\n\n    def docvalues(self, name: str, type=None) -&gt; DocValues.Sorted:\n        \"\"\"Return chained lucene DocValues, suitable for custom sorting or grouping.\n\n        Note multi-valued DocValues aren't thread-safe and only supported ordered iteration.\n\n        Args:\n            name: field name\n            type: int or float for converting values\n        \"\"\"\n        types = {int: int, float: util.NumericUtils.sortableLongToDouble}\n        type = types.get(type, util.BytesRef.utf8ToString)\n        docValuesType = str(self.fieldinfos[name].docValuesType).title().replace('_', '')\n        method = getattr(index.MultiDocValues, f'get{docValuesType}Values')\n        return getattr(DocValues, docValuesType)(method(self.indexReader, name), len(self), type)\n\n    def copy(\n        self, dest, query: search.Query = None, exclude: search.Query = None, merge: int = 0\n    ) -&gt; int:\n        \"\"\"Copy the index to the destination directory.\n\n        Optimized to use hard links if the destination is a file system path.\n\n        Args:\n            dest: destination directory path or lucene Directory\n            query: optional lucene Query to select documents\n            exclude: optional lucene Query to exclude documents\n            merge: optionally merge into maximum number of segments\n        \"\"\"\n        copy(self.indexCommit, dest)\n        with IndexWriter(dest) as writer:\n            if query:\n                writer.delete(Query.alldocs() - query)\n            if exclude:\n                writer.delete(exclude)\n            writer.commit()\n            writer.forceMergeDeletes()\n            if merge:\n                writer.forceMerge(merge)\n            return len(writer)\n\n    def terms(self, name: str, value='', stop='', counts=False, distance=0, prefix=0) -&gt; Iterator:\n        \"\"\"Generate a slice of term values, optionally with frequency counts.\n\n        Args:\n            name: field name\n            value: term prefix, start value (given stop), or fuzzy value (given distance)\n            stop: optional upper bound for range terms\n            counts: include frequency counts\n            distance: maximum edit distance for fuzzy terms\n            prefix: prefix length for fuzzy terms\n        \"\"\"\n        terms = index.MultiTerms.getTerms(self.indexReader, name)\n        if not terms:\n            return iter([])\n        term, termsenum = index.Term(name, value), terms.iterator()\n        if distance:\n            terms = termsenum = search.FuzzyTermsEnum(terms, term, distance, prefix, False)\n        else:\n            termsenum.seekCeil(util.BytesRef(value))\n            terms = itertools.chain([termsenum.term()], util.BytesRefIterator.cast_(termsenum))\n        terms = map(operator.methodcaller('utf8ToString'), terms)\n        predicate = (\n            partial(operator.gt, stop) if stop else operator.methodcaller('startswith', value)\n        )\n        if not distance:\n            terms = itertools.takewhile(predicate, terms)  # type: ignore\n        return ((term, termsenum.docFreq()) for term in terms) if counts else terms\n\n    def docs(self, name: str, value, counts=False) -&gt; Iterator:\n        \"\"\"Generate doc ids which contain given term, optionally with frequency counts.\"\"\"\n        docsenum = index.MultiTerms.getTermPostingsEnum(\n            self.indexReader, name, util.BytesRef(value)\n        )\n        docs = iter(docsenum.nextDoc, index.PostingsEnum.NO_MORE_DOCS) if docsenum else ()\n        return ((doc, docsenum.freq()) for doc in docs) if counts else iter(docs)  # type: ignore\n\n    def positions(self, name: str, value, payloads=False, offsets=False) -&gt; Iterator[tuple]:\n        \"\"\"Generate doc ids and positions which contain given term.\n\n        Optionally with offsets, or only ones with payloads.\"\"\"\n        docsenum = index.MultiTerms.getTermPostingsEnum(\n            self.indexReader, name, util.BytesRef(value)\n        )\n        for doc in iter(docsenum.nextDoc, index.PostingsEnum.NO_MORE_DOCS) if docsenum else ():\n            positions = (docsenum.nextPosition() for _ in range(docsenum.freq()))\n            if payloads:\n                positions = (\n                    (position, docsenum.payload.utf8ToString())\n                    for position in positions\n                    if docsenum.payload\n                )\n            elif offsets:\n                positions = (\n                    (docsenum.startOffset(), docsenum.endOffset()) for position in positions\n                )\n            yield doc, list(positions)\n\n    def vector(self, id, field):\n        terms = self.termVectors().get(id, field)\n        termsenum = terms.iterator() if terms else index.TermsEnum.EMPTY\n        terms = map(operator.methodcaller('utf8ToString'), util.BytesRefIterator.cast_(termsenum))\n        return termsenum, terms\n\n    def termvector(self, id: int, field: str, counts=False) -&gt; Iterator:\n        \"\"\"Generate terms for given doc id and field, optionally with frequency counts.\"\"\"\n        termsenum, terms = self.vector(id, field)\n        return ((term, termsenum.totalTermFreq()) for term in terms) if counts else terms\n\n    def positionvector(self, id: int, field: str, offsets=False) -&gt; Iterator[tuple]:\n        \"\"\"Generate terms and positions for given doc id and field, optionally with character offsets.\"\"\"\n        termsenum, terms = self.vector(id, field)\n        for term in terms:\n            docsenum = termsenum.postings(None)\n            assert 0 &lt;= docsenum.nextDoc() &lt; docsenum.NO_MORE_DOCS\n            positions = (docsenum.nextPosition() for _ in range(docsenum.freq()))\n            if offsets:\n                positions = ((docsenum.startOffset(), docsenum.endOffset()) for _ in positions)\n            yield term, list(positions)\n\n    def morelikethis(self, doc, *fields, **attrs) -&gt; Query:\n        \"\"\"Return MoreLikeThis query for document.\n\n        Args:\n            doc: document id or text\n            *fields: document fields to use, optional for termvectors\n            **attrs: additional attributes to set on the morelikethis object\n        \"\"\"\n        mlt = queries.mlt.MoreLikeThis(self.indexReader)\n        mlt.fieldNames = fields or None\n        for name, value in attrs.items():\n            setattr(mlt, name, value)\n        return mlt.like(fields[0], StringReader(doc)) if isinstance(doc, str) else mlt.like(doc)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.directory","title":"<code>directory</code>  <code>property</code>","text":"<p>reader's lucene Directory</p>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.fieldinfos","title":"<code>fieldinfos</code>  <code>property</code>","text":"<p>mapping of field names to lucene FieldInfos</p>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.path","title":"<code>path</code>  <code>property</code>","text":"<p>FSDirectory path</p>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.readers","title":"<code>readers</code>  <code>property</code>","text":"<p>segment readers</p>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.segments","title":"<code>segments</code>  <code>property</code>","text":"<p>segment filenames with document counts</p>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.timestamp","title":"<code>timestamp</code>  <code>property</code>","text":"<p>timestamp of reader's last commit</p>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.complete","title":"<code>complete(name, prefix, count)</code>","text":"<p>Return autocomplete suggestions for word prefix.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def complete(self, name: str, prefix: str, count: int) -&gt; list[str]:\n    \"\"\"Return autocomplete suggestions for word prefix.\"\"\"\n    terms = dict(self.terms(name, prefix, counts=True))\n    return heapq.nlargest(count, terms, key=terms.__getitem__)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.copy","title":"<code>copy(dest, query=None, exclude=None, merge=0)</code>","text":"<p>Copy the index to the destination directory.</p> <p>Optimized to use hard links if the destination is a file system path.</p> <p>Parameters:</p> Name Type Description Default <code>dest</code> <p>destination directory path or lucene Directory</p> required <code>query</code> <code>Query</code> <p>optional lucene Query to select documents</p> <code>None</code> <code>exclude</code> <code>Query</code> <p>optional lucene Query to exclude documents</p> <code>None</code> <code>merge</code> <code>int</code> <p>optionally merge into maximum number of segments</p> <code>0</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def copy(\n    self, dest, query: search.Query = None, exclude: search.Query = None, merge: int = 0\n) -&gt; int:\n    \"\"\"Copy the index to the destination directory.\n\n    Optimized to use hard links if the destination is a file system path.\n\n    Args:\n        dest: destination directory path or lucene Directory\n        query: optional lucene Query to select documents\n        exclude: optional lucene Query to exclude documents\n        merge: optionally merge into maximum number of segments\n    \"\"\"\n    copy(self.indexCommit, dest)\n    with IndexWriter(dest) as writer:\n        if query:\n            writer.delete(Query.alldocs() - query)\n        if exclude:\n            writer.delete(exclude)\n        writer.commit()\n        writer.forceMergeDeletes()\n        if merge:\n            writer.forceMerge(merge)\n        return len(writer)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.dictionary","title":"<code>dictionary(name, *args)</code>","text":"<p>Return lucene Dictionary, suitable for spellcheckers.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def dictionary(self, name: str, *args) -&gt; spell.Dictionary:\n    \"\"\"Return lucene Dictionary, suitable for spellcheckers.\"\"\"\n    cls = spell.HighFrequencyDictionary if args else spell.LuceneDictionary\n    return cls(self.indexReader, name, *args)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.docs","title":"<code>docs(name, value, counts=False)</code>","text":"<p>Generate doc ids which contain given term, optionally with frequency counts.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def docs(self, name: str, value, counts=False) -&gt; Iterator:\n    \"\"\"Generate doc ids which contain given term, optionally with frequency counts.\"\"\"\n    docsenum = index.MultiTerms.getTermPostingsEnum(\n        self.indexReader, name, util.BytesRef(value)\n    )\n    docs = iter(docsenum.nextDoc, index.PostingsEnum.NO_MORE_DOCS) if docsenum else ()\n    return ((doc, docsenum.freq()) for doc in docs) if counts else iter(docs)  # type: ignore\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.docvalues","title":"<code>docvalues(name, type=None)</code>","text":"<p>Return chained lucene DocValues, suitable for custom sorting or grouping.</p> <p>Note multi-valued DocValues aren't thread-safe and only supported ordered iteration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>field name</p> required <code>type</code> <p>int or float for converting values</p> <code>None</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def docvalues(self, name: str, type=None) -&gt; DocValues.Sorted:\n    \"\"\"Return chained lucene DocValues, suitable for custom sorting or grouping.\n\n    Note multi-valued DocValues aren't thread-safe and only supported ordered iteration.\n\n    Args:\n        name: field name\n        type: int or float for converting values\n    \"\"\"\n    types = {int: int, float: util.NumericUtils.sortableLongToDouble}\n    type = types.get(type, util.BytesRef.utf8ToString)\n    docValuesType = str(self.fieldinfos[name].docValuesType).title().replace('_', '')\n    method = getattr(index.MultiDocValues, f'get{docValuesType}Values')\n    return getattr(DocValues, docValuesType)(method(self.indexReader, name), len(self), type)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.morelikethis","title":"<code>morelikethis(doc, *fields, **attrs)</code>","text":"<p>Return MoreLikeThis query for document.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <p>document id or text</p> required <code>*fields</code> <p>document fields to use, optional for termvectors</p> <code>()</code> <code>**attrs</code> <p>additional attributes to set on the morelikethis object</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def morelikethis(self, doc, *fields, **attrs) -&gt; Query:\n    \"\"\"Return MoreLikeThis query for document.\n\n    Args:\n        doc: document id or text\n        *fields: document fields to use, optional for termvectors\n        **attrs: additional attributes to set on the morelikethis object\n    \"\"\"\n    mlt = queries.mlt.MoreLikeThis(self.indexReader)\n    mlt.fieldNames = fields or None\n    for name, value in attrs.items():\n        setattr(mlt, name, value)\n    return mlt.like(fields[0], StringReader(doc)) if isinstance(doc, str) else mlt.like(doc)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.positions","title":"<code>positions(name, value, payloads=False, offsets=False)</code>","text":"<p>Generate doc ids and positions which contain given term.</p> <p>Optionally with offsets, or only ones with payloads.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def positions(self, name: str, value, payloads=False, offsets=False) -&gt; Iterator[tuple]:\n    \"\"\"Generate doc ids and positions which contain given term.\n\n    Optionally with offsets, or only ones with payloads.\"\"\"\n    docsenum = index.MultiTerms.getTermPostingsEnum(\n        self.indexReader, name, util.BytesRef(value)\n    )\n    for doc in iter(docsenum.nextDoc, index.PostingsEnum.NO_MORE_DOCS) if docsenum else ():\n        positions = (docsenum.nextPosition() for _ in range(docsenum.freq()))\n        if payloads:\n            positions = (\n                (position, docsenum.payload.utf8ToString())\n                for position in positions\n                if docsenum.payload\n            )\n        elif offsets:\n            positions = (\n                (docsenum.startOffset(), docsenum.endOffset()) for position in positions\n            )\n        yield doc, list(positions)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.positionvector","title":"<code>positionvector(id, field, offsets=False)</code>","text":"<p>Generate terms and positions for given doc id and field, optionally with character offsets.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def positionvector(self, id: int, field: str, offsets=False) -&gt; Iterator[tuple]:\n    \"\"\"Generate terms and positions for given doc id and field, optionally with character offsets.\"\"\"\n    termsenum, terms = self.vector(id, field)\n    for term in terms:\n        docsenum = termsenum.postings(None)\n        assert 0 &lt;= docsenum.nextDoc() &lt; docsenum.NO_MORE_DOCS\n        positions = (docsenum.nextPosition() for _ in range(docsenum.freq()))\n        if offsets:\n            positions = ((docsenum.startOffset(), docsenum.endOffset()) for _ in positions)\n        yield term, list(positions)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.sortfield","title":"<code>sortfield(name, type=None, reverse=False)</code>","text":"<p>Return lucene SortField, deriving the the type from FieldInfos if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>field name</p> required <code>type</code> <p>int, float, or name compatible with SortField constants</p> <code>None</code> <code>reverse</code> <p>reverse flag used with sort</p> <code>False</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def sortfield(self, name: str, type=None, reverse=False) -&gt; search.SortField:\n    \"\"\"Return lucene SortField, deriving the the type from FieldInfos if necessary.\n\n    Args:\n        name: field name\n        type: int, float, or name compatible with SortField constants\n        reverse: reverse flag used with sort\n    \"\"\"\n    if type is None:\n        type = str(self.fieldinfos[name].docValuesType)\n    type = Field.types.get(type, type).upper()\n    return search.SortField(name, getattr(search.SortField.Type, type), reverse)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.suggest","title":"<code>suggest(name, value, count=1, **attrs)</code>","text":"<p>Return spelling suggestions from DirectSpellChecker.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>field name</p> required <code>value</code> <p>term</p> required <code>count</code> <code>int</code> <p>maximum number of suggestions</p> <code>1</code> <code>**attrs</code> <p>DirectSpellChecker options</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def suggest(self, name: str, value, count: int = 1, **attrs) -&gt; list:\n    \"\"\"Return spelling suggestions from DirectSpellChecker.\n\n    Args:\n        name: field name\n        value: term\n        count: maximum number of suggestions\n        **attrs: DirectSpellChecker options\n    \"\"\"\n    checker = spell.DirectSpellChecker()\n    for attr in attrs:\n        setattr(checker, attr, attrs[attr])\n    words = checker.suggestSimilar(index.Term(name, value), count, self.indexReader)\n    return [word.string for word in words]\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.terms","title":"<code>terms(name, value='', stop='', counts=False, distance=0, prefix=0)</code>","text":"<p>Generate a slice of term values, optionally with frequency counts.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>field name</p> required <code>value</code> <p>term prefix, start value (given stop), or fuzzy value (given distance)</p> <code>''</code> <code>stop</code> <p>optional upper bound for range terms</p> <code>''</code> <code>counts</code> <p>include frequency counts</p> <code>False</code> <code>distance</code> <p>maximum edit distance for fuzzy terms</p> <code>0</code> <code>prefix</code> <p>prefix length for fuzzy terms</p> <code>0</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def terms(self, name: str, value='', stop='', counts=False, distance=0, prefix=0) -&gt; Iterator:\n    \"\"\"Generate a slice of term values, optionally with frequency counts.\n\n    Args:\n        name: field name\n        value: term prefix, start value (given stop), or fuzzy value (given distance)\n        stop: optional upper bound for range terms\n        counts: include frequency counts\n        distance: maximum edit distance for fuzzy terms\n        prefix: prefix length for fuzzy terms\n    \"\"\"\n    terms = index.MultiTerms.getTerms(self.indexReader, name)\n    if not terms:\n        return iter([])\n    term, termsenum = index.Term(name, value), terms.iterator()\n    if distance:\n        terms = termsenum = search.FuzzyTermsEnum(terms, term, distance, prefix, False)\n    else:\n        termsenum.seekCeil(util.BytesRef(value))\n        terms = itertools.chain([termsenum.term()], util.BytesRefIterator.cast_(termsenum))\n    terms = map(operator.methodcaller('utf8ToString'), terms)\n    predicate = (\n        partial(operator.gt, stop) if stop else operator.methodcaller('startswith', value)\n    )\n    if not distance:\n        terms = itertools.takewhile(predicate, terms)  # type: ignore\n    return ((term, termsenum.docFreq()) for term in terms) if counts else terms\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexReader.termvector","title":"<code>termvector(id, field, counts=False)</code>","text":"<p>Generate terms for given doc id and field, optionally with frequency counts.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def termvector(self, id: int, field: str, counts=False) -&gt; Iterator:\n    \"\"\"Generate terms for given doc id and field, optionally with frequency counts.\"\"\"\n    termsenum, terms = self.vector(id, field)\n    return ((term, termsenum.totalTermFreq()) for term in terms) if counts else terms\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher","title":"<code>lupyne.engine.indexers.IndexSearcher</code>","text":"<p>               Bases: <code>IndexSearcher</code>, <code>IndexReader</code></p> <p>Inherited lucene IndexSearcher, with a mixed-in IndexReader.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <p>directory path, lucene Directory, or lucene IndexReader</p> required <code>analyzer</code> <p>lucene Analyzer, default StandardAnalyzer</p> <code>None</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>class IndexSearcher(search.IndexSearcher, IndexReader):\n    \"\"\"Inherited lucene IndexSearcher, with a mixed-in IndexReader.\n\n    Args:\n        directory: directory path, lucene Directory, or lucene IndexReader\n        analyzer: lucene Analyzer, default StandardAnalyzer\n    \"\"\"\n\n    def __init__(self, directory, analyzer=None):\n        self.shared = closing()\n        super().__init__(self.shared.reader(directory))\n        self.analyzer = self.shared.analyzer(analyzer)\n\n    def __del__(self):\n        if hash(self):  # pragma: no branch\n            self.decRef()\n\n    def openIfChanged(self):\n        return index.DirectoryReader.openIfChanged(index.DirectoryReader.cast_(self.indexReader))\n\n    def reopen(self) -&gt; 'IndexSearcher':\n        \"\"\"Return current [IndexSearcher][lupyne.engine.indexers.IndexSearcher].\n\n        Only creates a new one if necessary.\n        \"\"\"\n        reader = self.openIfChanged()\n        if reader is None:\n            return self\n        other = type(self)(reader, self.analyzer)\n        other.decRef()\n        other.shared = self.shared\n        return other\n\n    def __getitem__(self, id: int) -&gt; Document:\n        return Document(self.storedFields().document(id))\n\n    def get(self, id: int, *fields: str) -&gt; Document:\n        \"\"\"Return [Document][lupyne.engine.documents.Document] with only selected fields loaded.\"\"\"\n        return Document(self.storedFields().document(id, HashSet(Arrays.asList(fields))))\n\n    def spans(self, query: spans.SpanQuery, positions=False) -&gt; Iterator[tuple]:\n        \"\"\"Generate docs with occurrence counts for a span query.\n\n        Args:\n            query: lucene SpanQuery\n            positions: optionally include slice positions instead of counts\n        \"\"\"\n        offset = 0\n        weight = query.createWeight(self, search.ScoreMode.COMPLETE_NO_SCORES, 1.0)\n        postings = queries.spans.SpanWeight.Postings.POSITIONS\n        for reader in self.readers:\n            try:\n                spans = weight.getSpans(reader.context, postings)\n            except lucene.JavaError:  # EOF\n                continue\n            for doc in iter(spans.nextDoc, spans.NO_MORE_DOCS):\n                starts = iter(spans.nextStartPosition, spans.NO_MORE_POSITIONS)\n                if positions:\n                    values = [(start, spans.endPosition()) for start in starts]\n                else:\n                    values = sum(1 for _ in starts)  # type: ignore\n                yield (doc + offset), values\n            offset += reader.maxDoc()\n\n    def parse(self, query, spellcheck=False, **kwargs) -&gt; search.Query:\n        if isinstance(query, search.Query):\n            return query\n        if spellcheck:\n            kwargs['parser'], kwargs['searcher'] = SpellParser, self\n        return Analyzer.parse(self.analyzer, query, **kwargs)\n\n    @property\n    def highlighter(self) -&gt; uhighlight.UnifiedHighlighter:\n        \"\"\"lucene UnifiedHighlighter\"\"\"\n        return uhighlight.UnifiedHighlighter(self, self.analyzer)\n\n    def count(self, *query, **options) -&gt; int:\n        \"\"\"Return number of hits for given query or term.\n\n        Args:\n            *query: [search][lupyne.engine.indexers.IndexSearcher.search] compatible query, or optimally a name and value\n            **options: additional [search][lupyne.engine.indexers.IndexSearcher.search] options\n        \"\"\"\n        if len(query) &gt; 1:\n            return self.docFreq(index.Term(*query))\n        return super().count(self.parse(*query, **options) if query else Query.alldocs())\n\n    def collector(self, count=None, sort=None, reverse=False, scores=False, mincount=1000):\n        if count is None:\n            return search.CachingCollector.create(True, float('inf'))\n        count = min(count, self.maxDoc() or 1)\n        mincount = max(count, mincount)\n        if sort is None:\n            return search.TopScoreDocCollectorManager(count, mincount).newCollector()\n        if isinstance(sort, str):\n            sort = self.sortfield(sort, reverse=reverse)\n        if not isinstance(sort, search.Sort):\n            sort = search.Sort(sort)\n        return search.TopFieldCollectorManager(sort, count, mincount).newCollector()\n\n    def search(\n        self,\n        query=None,\n        count=None,\n        sort=None,\n        reverse=False,\n        scores=False,\n        mincount=1000,\n        **parser,\n    ) -&gt; Hits:\n        \"\"\"Run query and return [Hits][lupyne.engine.documents.Hits].\n\n        Note:\n            changed in version 2.3: maxscore option removed; use Hits.maxscore property\n\n        Args:\n            query: query string or lucene Query\n            count: maximum number of hits to retrieve\n            sort: lucene Sort parameters\n            reverse: reverse flag used with sort\n            scores: compute scores for candidate results when sorting\n            mincount: total hit count accuracy threshold\n            **parser: [parse][lupyne.engine.analyzers.Analyzer.parse]` options\n        \"\"\"\n        query = Query.alldocs() if query is None else self.parse(query, **parser)\n        results = cache = collector = self.collector(count, sort, reverse, scores, mincount)\n        super().search(query, results)\n        if isinstance(cache, search.CachingCollector):\n            collector = search.TotalHitCountCollector()\n            cache.replay(collector)\n            count = collector.totalHits or 1\n            collector = self.collector(count, sort, reverse, scores, count)\n            cache.replay(collector)\n        topdocs = collector.topDocs()\n        if scores:\n            search.TopFieldCollector.populateScores(topdocs.scoreDocs, self, query)\n        return Hits(self, topdocs.scoreDocs, topdocs.totalHits)\n\n    def facets(self, query, *fields: str, **query_map: dict) -&gt; dict:\n        \"\"\"Return mapping of document counts for the intersection with each facet.\n\n        Args:\n            query: query string or lucene Query\n            *fields: field names for lucene GroupingSearch\n            **query_map: `{facet: {key: query, ...}, ...}` for intersected query counts\n        \"\"\"\n        query = self.parse(query)\n        counts = {field: self.groupby(field, query).facets for field in fields}\n        for facet, values in query_map.items():\n            counts[facet] = {key: self.count(Query.all(query, values[key])) for key in values}\n        return counts\n\n    def groupby(\n        self, field: str, query, count: int | None = None, start: int = 0, **attrs\n    ) -&gt; Groups:\n        \"\"\"Return [Hits][lupyne.engine.documents.Hits] grouped by field\n        using a [GroupingSearch][lupyne.engine.documents.GroupingSearch].\"\"\"\n        return GroupingSearch(field, **attrs).search(self, self.parse(query), count, start)\n\n    def match(self, document: Mapping, *queries) -&gt; Iterator[float]:\n        \"\"\"Generate scores for all queries against a given document mapping.\"\"\"\n        searcher = index.memory.MemoryIndex()\n        for name, value in document.items():\n            args = [self.analyzer] * isinstance(value, str)\n            searcher.addField(name, value, *args)\n        return (searcher.search(self.parse(query)) for query in queries)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.highlighter","title":"<code>highlighter</code>  <code>property</code>","text":"<p>lucene UnifiedHighlighter</p>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.count","title":"<code>count(*query, **options)</code>","text":"<p>Return number of hits for given query or term.</p> <p>Parameters:</p> Name Type Description Default <code>*query</code> <p>search compatible query, or optimally a name and value</p> <code>()</code> <code>**options</code> <p>additional search options</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def count(self, *query, **options) -&gt; int:\n    \"\"\"Return number of hits for given query or term.\n\n    Args:\n        *query: [search][lupyne.engine.indexers.IndexSearcher.search] compatible query, or optimally a name and value\n        **options: additional [search][lupyne.engine.indexers.IndexSearcher.search] options\n    \"\"\"\n    if len(query) &gt; 1:\n        return self.docFreq(index.Term(*query))\n    return super().count(self.parse(*query, **options) if query else Query.alldocs())\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.facets","title":"<code>facets(query, *fields, **query_map)</code>","text":"<p>Return mapping of document counts for the intersection with each facet.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query string or lucene Query</p> required <code>*fields</code> <code>str</code> <p>field names for lucene GroupingSearch</p> <code>()</code> <code>**query_map</code> <code>dict</code> <p><code>{facet: {key: query, ...}, ...}</code> for intersected query counts</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def facets(self, query, *fields: str, **query_map: dict) -&gt; dict:\n    \"\"\"Return mapping of document counts for the intersection with each facet.\n\n    Args:\n        query: query string or lucene Query\n        *fields: field names for lucene GroupingSearch\n        **query_map: `{facet: {key: query, ...}, ...}` for intersected query counts\n    \"\"\"\n    query = self.parse(query)\n    counts = {field: self.groupby(field, query).facets for field in fields}\n    for facet, values in query_map.items():\n        counts[facet] = {key: self.count(Query.all(query, values[key])) for key in values}\n    return counts\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.get","title":"<code>get(id, *fields)</code>","text":"<p>Return Document with only selected fields loaded.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def get(self, id: int, *fields: str) -&gt; Document:\n    \"\"\"Return [Document][lupyne.engine.documents.Document] with only selected fields loaded.\"\"\"\n    return Document(self.storedFields().document(id, HashSet(Arrays.asList(fields))))\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.groupby","title":"<code>groupby(field, query, count=None, start=0, **attrs)</code>","text":"<p>Return Hits grouped by field using a GroupingSearch.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def groupby(\n    self, field: str, query, count: int | None = None, start: int = 0, **attrs\n) -&gt; Groups:\n    \"\"\"Return [Hits][lupyne.engine.documents.Hits] grouped by field\n    using a [GroupingSearch][lupyne.engine.documents.GroupingSearch].\"\"\"\n    return GroupingSearch(field, **attrs).search(self, self.parse(query), count, start)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.match","title":"<code>match(document, *queries)</code>","text":"<p>Generate scores for all queries against a given document mapping.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def match(self, document: Mapping, *queries) -&gt; Iterator[float]:\n    \"\"\"Generate scores for all queries against a given document mapping.\"\"\"\n    searcher = index.memory.MemoryIndex()\n    for name, value in document.items():\n        args = [self.analyzer] * isinstance(value, str)\n        searcher.addField(name, value, *args)\n    return (searcher.search(self.parse(query)) for query in queries)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.reopen","title":"<code>reopen()</code>","text":"<p>Return current IndexSearcher.</p> <p>Only creates a new one if necessary.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def reopen(self) -&gt; 'IndexSearcher':\n    \"\"\"Return current [IndexSearcher][lupyne.engine.indexers.IndexSearcher].\n\n    Only creates a new one if necessary.\n    \"\"\"\n    reader = self.openIfChanged()\n    if reader is None:\n        return self\n    other = type(self)(reader, self.analyzer)\n    other.decRef()\n    other.shared = self.shared\n    return other\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.search","title":"<code>search(query=None, count=None, sort=None, reverse=False, scores=False, mincount=1000, **parser)</code>","text":"<p>Run query and return Hits.</p> Note <p>changed in version 2.3: maxscore option removed; use Hits.maxscore property</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query string or lucene Query</p> <code>None</code> <code>count</code> <p>maximum number of hits to retrieve</p> <code>None</code> <code>sort</code> <p>lucene Sort parameters</p> <code>None</code> <code>reverse</code> <p>reverse flag used with sort</p> <code>False</code> <code>scores</code> <p>compute scores for candidate results when sorting</p> <code>False</code> <code>mincount</code> <p>total hit count accuracy threshold</p> <code>1000</code> <code>**parser</code> <p>parse` options</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def search(\n    self,\n    query=None,\n    count=None,\n    sort=None,\n    reverse=False,\n    scores=False,\n    mincount=1000,\n    **parser,\n) -&gt; Hits:\n    \"\"\"Run query and return [Hits][lupyne.engine.documents.Hits].\n\n    Note:\n        changed in version 2.3: maxscore option removed; use Hits.maxscore property\n\n    Args:\n        query: query string or lucene Query\n        count: maximum number of hits to retrieve\n        sort: lucene Sort parameters\n        reverse: reverse flag used with sort\n        scores: compute scores for candidate results when sorting\n        mincount: total hit count accuracy threshold\n        **parser: [parse][lupyne.engine.analyzers.Analyzer.parse]` options\n    \"\"\"\n    query = Query.alldocs() if query is None else self.parse(query, **parser)\n    results = cache = collector = self.collector(count, sort, reverse, scores, mincount)\n    super().search(query, results)\n    if isinstance(cache, search.CachingCollector):\n        collector = search.TotalHitCountCollector()\n        cache.replay(collector)\n        count = collector.totalHits or 1\n        collector = self.collector(count, sort, reverse, scores, count)\n        cache.replay(collector)\n    topdocs = collector.topDocs()\n    if scores:\n        search.TopFieldCollector.populateScores(topdocs.scoreDocs, self, query)\n    return Hits(self, topdocs.scoreDocs, topdocs.totalHits)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexSearcher.spans","title":"<code>spans(query, positions=False)</code>","text":"<p>Generate docs with occurrence counts for a span query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>SpanQuery</code> <p>lucene SpanQuery</p> required <code>positions</code> <p>optionally include slice positions instead of counts</p> <code>False</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def spans(self, query: spans.SpanQuery, positions=False) -&gt; Iterator[tuple]:\n    \"\"\"Generate docs with occurrence counts for a span query.\n\n    Args:\n        query: lucene SpanQuery\n        positions: optionally include slice positions instead of counts\n    \"\"\"\n    offset = 0\n    weight = query.createWeight(self, search.ScoreMode.COMPLETE_NO_SCORES, 1.0)\n    postings = queries.spans.SpanWeight.Postings.POSITIONS\n    for reader in self.readers:\n        try:\n            spans = weight.getSpans(reader.context, postings)\n        except lucene.JavaError:  # EOF\n            continue\n        for doc in iter(spans.nextDoc, spans.NO_MORE_DOCS):\n            starts = iter(spans.nextStartPosition, spans.NO_MORE_POSITIONS)\n            if positions:\n                values = [(start, spans.endPosition()) for start in starts]\n            else:\n                values = sum(1 for _ in starts)  # type: ignore\n            yield (doc + offset), values\n        offset += reader.maxDoc()\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.MultiSearcher","title":"<code>lupyne.engine.indexers.MultiSearcher</code>","text":"<p>               Bases: <code>IndexSearcher</code></p> <p>IndexSearcher with underlying lucene MultiReader.</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <p>directory paths, Directories, IndexReaders, or a single MultiReader</p> required <code>analyzer</code> <p>lucene Analyzer, default StandardAnalyzer</p> <code>None</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>class MultiSearcher(IndexSearcher):\n    \"\"\"IndexSearcher with underlying lucene MultiReader.\n\n    Args:\n        reader: directory paths, Directories, IndexReaders, or a single MultiReader\n        analyzer: lucene Analyzer, default StandardAnalyzer\n    \"\"\"\n\n    def __init__(self, reader, analyzer=None):\n        super().__init__(reader, analyzer)\n        self.indexReaders = [\n            index.DirectoryReader.cast_(context.reader()) for context in self.context.children()\n        ]\n        self.version = sum(reader.version for reader in self.indexReaders)\n\n    def __getattr__(self, name):\n        return getattr(index.MultiReader.cast_(self.indexReader), name)\n\n    def openIfChanged(self):\n        readers = list(map(index.DirectoryReader.openIfChanged, self.indexReaders))\n        if any(readers):\n            readers = [new or old.incRef() or old for new, old in zip(readers, self.indexReaders)]\n            return index.MultiReader(readers)\n\n    @property\n    def timestamp(self):\n        return max(IndexReader(reader).timestamp for reader in self.indexReaders)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter","title":"<code>lupyne.engine.indexers.IndexWriter</code>","text":"<p>               Bases: <code>IndexWriter</code></p> <p>Inherited lucene IndexWriter.</p> <p>Supports setting fields parameters explicitly, so documents can be represented as dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <p>directory path or lucene Directory</p> required <code>mode</code> <code>str</code> <p>file mode (rwa), except updating (+) is implied</p> <code>'a'</code> <code>analyzer</code> <p>lucene Analyzer, default StandardAnalyzer</p> <code>None</code> <code>version</code> <p>lucene Version argument passed to IndexWriterConfig, default is latest</p> <code>None</code> <code>**attrs</code> <p>additional attributes to set on IndexWriterConfig</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>class IndexWriter(index.IndexWriter):\n    \"\"\"Inherited lucene IndexWriter.\n\n    Supports setting fields parameters explicitly, so documents can be represented as dictionaries.\n\n    Args:\n        directory: directory path or lucene Directory\n        mode: file mode (rwa), except updating (+) is implied\n        analyzer: lucene Analyzer, default StandardAnalyzer\n        version: lucene Version argument passed to IndexWriterConfig, default is latest\n        **attrs: additional attributes to set on IndexWriterConfig\n    \"\"\"\n\n    parse = IndexSearcher.parse\n\n    def __init__(self, directory, mode: str = 'a', analyzer=None, version=None, **attrs):\n        self.shared = closing()\n        args = [] if analyzer is None else [self.shared.analyzer(analyzer)]\n        config = index.IndexWriterConfig(*args)\n        config.openMode = index.IndexWriterConfig.OpenMode.values()['wra'.index(mode)]\n        for name, value in attrs.items():\n            setattr(config, name, value)\n        self.policy = index.SnapshotDeletionPolicy(config.indexDeletionPolicy)\n        config.indexDeletionPolicy = self.policy\n        super().__init__(self.shared.directory(directory), config)\n        self.fields: dict = {}\n\n    def __del__(self):\n        if hash(self):\n            with suppress(IOException):\n                self.close()\n\n    def __len__(self):\n        return self.docStats.numDocs\n\n    @classmethod\n    def check(cls, directory, repair=False) -&gt; index.CheckIndex.Status:\n        \"\"\"Check and optionally fix unlocked index, returning lucene CheckIndex.Status.\"\"\"\n        with closing.store(directory) as directory:\n            with contextlib.closing(index.CheckIndex(directory)) as checkindex:\n                status = checkindex.checkIndex()\n                if repair:\n                    checkindex.exorciseIndex(status)\n        return status\n\n    def set(self, name: str, cls=Field, **settings) -&gt; Field:\n        \"\"\"Assign settings to field name and return the field.\n\n        Args:\n            name: registered name of field\n            cls: optional [Field][lupyne.engine.documents.Field] constructor\n            **settings: stored, indexed, etc. options compatible with [Field][lupyne.engine.documents.Field]\n        \"\"\"\n        field = self.fields[name] = cls(name, **settings)\n        return field\n\n    def document(self, items=(), **terms) -&gt; document.Document:\n        \"\"\"Return lucene Document from mapping of field names to one or multiple values.\"\"\"\n        doc = document.Document()\n        for name, values in dict(items, **terms).items():\n            if isinstance(values, Atomic):\n                values = (values,)\n            for field in self.fields[name].items(*values):\n                doc.add(field)\n        return doc\n\n    def add(self, document=(), **terms):\n        \"\"\"Add [document][lupyne.engine.indexers.IndexWriter.document] to index with optional boost.\"\"\"\n        self.addDocument(self.document(document, **terms))\n\n    def update(self, name: str, value='', document=(), **terms):\n        \"\"\"Atomically delete documents which match given term\n        and add the new [document][lupyne.engine.indexers.IndexWriter.document].\"\"\"\n        doc = self.document(document, **terms)\n        term = index.Term(name, *[value] if value else doc.getValues(name))\n        fields = list(doc.iterator())\n        noindex = index.IndexOptions.NONE\n        for field in fields:\n            ft = Field.cast_(field.fieldType())\n            if ft.stored() or ft.indexOptions() != noindex or Field.dimensions.fget(ft):  # type: ignore\n                self.updateDocument(term, doc)\n                return\n        if fields:\n            self.updateDocValues(term, *fields)\n\n    def delete(self, *query, **options):\n        \"\"\"Remove documents which match given query or term.\n\n        Args:\n            *query: [search][lupyne.engine.indexers.IndexSearcher.search] compatible query, or optimally a name and value\n            **options: additional [parse][lupyne.engine.analyzers.Analyzer.parse] options\n        \"\"\"\n        parse = self.parse if len(query) == 1 else index.Term\n        self.deleteDocuments(parse(*query, **options))\n\n    def __iadd__(self, directory):\n        \"\"\"Add directory (or reader, searcher, writer) to index.\"\"\"\n        with closing.store(getattr(directory, 'directory', directory)) as directory:\n            self.addIndexes([directory])\n        return self\n\n    @contextlib.contextmanager\n    def snapshot(self):\n        \"\"\"Return context manager of an index commit snapshot.\"\"\"\n        commit = self.policy.snapshot()\n        try:\n            yield commit\n        finally:\n            self.policy.release(commit)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        if any(args):\n            self.rollback()\n        else:\n            self.commit()\n        self.close()\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.__iadd__","title":"<code>__iadd__(directory)</code>","text":"<p>Add directory (or reader, searcher, writer) to index.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def __iadd__(self, directory):\n    \"\"\"Add directory (or reader, searcher, writer) to index.\"\"\"\n    with closing.store(getattr(directory, 'directory', directory)) as directory:\n        self.addIndexes([directory])\n    return self\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.add","title":"<code>add(document=(), **terms)</code>","text":"<p>Add document to index with optional boost.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def add(self, document=(), **terms):\n    \"\"\"Add [document][lupyne.engine.indexers.IndexWriter.document] to index with optional boost.\"\"\"\n    self.addDocument(self.document(document, **terms))\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.check","title":"<code>check(directory, repair=False)</code>  <code>classmethod</code>","text":"<p>Check and optionally fix unlocked index, returning lucene CheckIndex.Status.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>@classmethod\ndef check(cls, directory, repair=False) -&gt; index.CheckIndex.Status:\n    \"\"\"Check and optionally fix unlocked index, returning lucene CheckIndex.Status.\"\"\"\n    with closing.store(directory) as directory:\n        with contextlib.closing(index.CheckIndex(directory)) as checkindex:\n            status = checkindex.checkIndex()\n            if repair:\n                checkindex.exorciseIndex(status)\n    return status\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.delete","title":"<code>delete(*query, **options)</code>","text":"<p>Remove documents which match given query or term.</p> <p>Parameters:</p> Name Type Description Default <code>*query</code> <p>search compatible query, or optimally a name and value</p> <code>()</code> <code>**options</code> <p>additional parse options</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def delete(self, *query, **options):\n    \"\"\"Remove documents which match given query or term.\n\n    Args:\n        *query: [search][lupyne.engine.indexers.IndexSearcher.search] compatible query, or optimally a name and value\n        **options: additional [parse][lupyne.engine.analyzers.Analyzer.parse] options\n    \"\"\"\n    parse = self.parse if len(query) == 1 else index.Term\n    self.deleteDocuments(parse(*query, **options))\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.document","title":"<code>document(items=(), **terms)</code>","text":"<p>Return lucene Document from mapping of field names to one or multiple values.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def document(self, items=(), **terms) -&gt; document.Document:\n    \"\"\"Return lucene Document from mapping of field names to one or multiple values.\"\"\"\n    doc = document.Document()\n    for name, values in dict(items, **terms).items():\n        if isinstance(values, Atomic):\n            values = (values,)\n        for field in self.fields[name].items(*values):\n            doc.add(field)\n    return doc\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.set","title":"<code>set(name, cls=Field, **settings)</code>","text":"<p>Assign settings to field name and return the field.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>registered name of field</p> required <code>cls</code> <p>optional Field constructor</p> <code>Field</code> <code>**settings</code> <p>stored, indexed, etc. options compatible with Field</p> <code>{}</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def set(self, name: str, cls=Field, **settings) -&gt; Field:\n    \"\"\"Assign settings to field name and return the field.\n\n    Args:\n        name: registered name of field\n        cls: optional [Field][lupyne.engine.documents.Field] constructor\n        **settings: stored, indexed, etc. options compatible with [Field][lupyne.engine.documents.Field]\n    \"\"\"\n    field = self.fields[name] = cls(name, **settings)\n    return field\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.snapshot","title":"<code>snapshot()</code>","text":"<p>Return context manager of an index commit snapshot.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>@contextlib.contextmanager\ndef snapshot(self):\n    \"\"\"Return context manager of an index commit snapshot.\"\"\"\n    commit = self.policy.snapshot()\n    try:\n        yield commit\n    finally:\n        self.policy.release(commit)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.IndexWriter.update","title":"<code>update(name, value='', document=(), **terms)</code>","text":"<p>Atomically delete documents which match given term and add the new document.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def update(self, name: str, value='', document=(), **terms):\n    \"\"\"Atomically delete documents which match given term\n    and add the new [document][lupyne.engine.indexers.IndexWriter.document].\"\"\"\n    doc = self.document(document, **terms)\n    term = index.Term(name, *[value] if value else doc.getValues(name))\n    fields = list(doc.iterator())\n    noindex = index.IndexOptions.NONE\n    for field in fields:\n        ft = Field.cast_(field.fieldType())\n        if ft.stored() or ft.indexOptions() != noindex or Field.dimensions.fget(ft):  # type: ignore\n            self.updateDocument(term, doc)\n            return\n    if fields:\n        self.updateDocValues(term, *fields)\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.Indexer","title":"<code>lupyne.engine.indexers.Indexer</code>","text":"<p>               Bases: <code>IndexWriter</code></p> <p>An all-purpose interface to an index.</p> <p>Creates an IndexWriter with a delegated IndexSearcher.</p> <p>Parameters:</p> Name Type Description Default <code>nrt</code> <p>optionally use a near real-time searcher</p> <code>False</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>class Indexer(IndexWriter):\n    \"\"\"An all-purpose interface to an index.\n\n    Creates an [IndexWriter][lupyne.engine.indexers.IndexWriter]\n    with a delegated [IndexSearcher][lupyne.engine.indexers.IndexSearcher].\n\n    Args:\n        nrt: optionally use a near real-time searcher\n    \"\"\"\n\n    def __init__(self, directory, mode='a', analyzer=None, version=None, nrt=False, **attrs):\n        super().__init__(directory, mode, analyzer, version, **attrs)\n        super().commit()\n        self.nrt = nrt\n        self.indexSearcher = IndexSearcher(self if nrt else self.directory, self.analyzer)\n\n    def __getattr__(self, name):\n        if name == 'indexSearcher':\n            raise AttributeError(name)\n        return getattr(self.indexSearcher, name)\n\n    def __contains__(self, id):\n        return id in self.indexSearcher\n\n    def __iter__(self):\n        return iter(self.indexSearcher)\n\n    def __getitem__(self, id):\n        return self.indexSearcher[id]\n\n    def refresh(self):\n        \"\"\"Store refreshed searcher from [reopening][lupyne.engine.indexers.IndexSearcher.reopen].\"\"\"\n        self.indexSearcher = self.indexSearcher.reopen()\n\n    def commit(self, merge: int = False):\n        \"\"\"Commit writes and [refresh][lupyne.engine.indexers.Indexer.refresh] searcher.\n\n        Args:\n            merge: merge segments with deletes, or optionally specify maximum number of segments\n        \"\"\"\n        super().commit()\n        if merge:\n            if isinstance(merge, bool):\n                self.forceMergeDeletes()\n            else:\n                self.forceMerge(merge)\n            super().commit()\n        self.refresh()\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.Indexer.commit","title":"<code>commit(merge=False)</code>","text":"<p>Commit writes and refresh searcher.</p> <p>Parameters:</p> Name Type Description Default <code>merge</code> <code>int</code> <p>merge segments with deletes, or optionally specify maximum number of segments</p> <code>False</code> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def commit(self, merge: int = False):\n    \"\"\"Commit writes and [refresh][lupyne.engine.indexers.Indexer.refresh] searcher.\n\n    Args:\n        merge: merge segments with deletes, or optionally specify maximum number of segments\n    \"\"\"\n    super().commit()\n    if merge:\n        if isinstance(merge, bool):\n            self.forceMergeDeletes()\n        else:\n            self.forceMerge(merge)\n        super().commit()\n    self.refresh()\n</code></pre>"},{"location":"engine/#lupyne.engine.indexers.Indexer.refresh","title":"<code>refresh()</code>","text":"<p>Store refreshed searcher from reopening.</p> Source code in <code>lupyne/engine/indexers.py</code> <pre><code>def refresh(self):\n    \"\"\"Store refreshed searcher from [reopening][lupyne.engine.indexers.IndexSearcher.reopen].\"\"\"\n    self.indexSearcher = self.indexSearcher.reopen()\n</code></pre>"},{"location":"engine/#documents","title":"documents","text":""},{"location":"engine/#lupyne.engine.documents.Document","title":"<code>lupyne.engine.documents.Document</code>","text":"<p>               Bases: <code>dict</code></p> <p>Multimapping of field names to values, but default getters return the first value.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class Document(dict):\n    \"\"\"Multimapping of field names to values, but default getters return the first value.\"\"\"\n\n    def __init__(self, doc: document.Document):\n        for field in doc.iterator():\n            value = convert(field.numericValue() or field.stringValue() or field.binaryValue())\n            self.setdefault(field.name(), []).append(value)\n\n    def __getitem__(self, name):\n        return super().__getitem__(name)[0]\n\n    def get(self, name: str, default=None):\n        return super().get(name, [default])[0]\n\n    def getlist(self, name: str) -&gt; list:\n        \"\"\"Return list of all values for given field.\"\"\"\n        return super().get(name, [])\n\n    def dict(self, *names: str, **defaults) -&gt; dict:\n        \"\"\"Return dict representation of document.\n\n        Args:\n            *names: names of multi-valued fields to return as a list\n            **defaults: include only given fields, using default values as necessary\n        \"\"\"\n        defaults |= {name: self[name] for name in (defaults or self) if name in self}\n        return defaults | {name: self.getlist(name) for name in names}\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Document.dict","title":"<code>dict(*names, **defaults)</code>","text":"<p>Return dict representation of document.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>names of multi-valued fields to return as a list</p> <code>()</code> <code>**defaults</code> <p>include only given fields, using default values as necessary</p> <code>{}</code> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def dict(self, *names: str, **defaults) -&gt; dict:\n    \"\"\"Return dict representation of document.\n\n    Args:\n        *names: names of multi-valued fields to return as a list\n        **defaults: include only given fields, using default values as necessary\n    \"\"\"\n    defaults |= {name: self[name] for name in (defaults or self) if name in self}\n    return defaults | {name: self.getlist(name) for name in names}\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Document.getlist","title":"<code>getlist(name)</code>","text":"<p>Return list of all values for given field.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def getlist(self, name: str) -&gt; list:\n    \"\"\"Return list of all values for given field.\"\"\"\n    return super().get(name, [])\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hit","title":"<code>lupyne.engine.documents.Hit</code>","text":"<p>               Bases: <code>Document</code></p> <p>A Document from a search result, with :attr:<code>id</code>, :attr:<code>score</code>, and optional :attr:<code>sortkeys</code>.</p> Note <p>changed in version 2.4: keys renamed to :attr:<code>sortkeys</code></p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class Hit(Document):\n    \"\"\"A Document from a search result, with :attr:`id`, :attr:`score`, and optional :attr:`sortkeys`.\n\n    Note:\n        changed in version 2.4: keys renamed to :attr:`sortkeys`\n    \"\"\"\n\n    def __init__(self, doc: document.Document, id: int, score: float, sortkeys=()):\n        super().__init__(doc)\n        self.id, self.score = id, score\n        self.sortkeys = tuple(map(convert, sortkeys))\n\n    def dict(self, *names: str, **defaults) -&gt; dict:\n        \"\"\"Return dict representation of document with __id__, __score__, and any sort __keys__.\"\"\"\n        result = super().dict(*names, **defaults)\n        result.update(__id__=self.id, __score__=self.score)\n        if self.sortkeys:\n            result['__sortkeys__'] = self.sortkeys\n        return result\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hit.dict","title":"<code>dict(*names, **defaults)</code>","text":"<p>Return dict representation of document with id, score, and any sort keys.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def dict(self, *names: str, **defaults) -&gt; dict:\n    \"\"\"Return dict representation of document with __id__, __score__, and any sort __keys__.\"\"\"\n    result = super().dict(*names, **defaults)\n    result.update(__id__=self.id, __score__=self.score)\n    if self.sortkeys:\n        result['__sortkeys__'] = self.sortkeys\n    return result\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits","title":"<code>lupyne.engine.documents.Hits</code>","text":"<p>Search results: lazily evaluated and memory efficient.</p> <p>Provides a read-only sequence interface to hit objects.</p> Note <p>changed in version 2.3: maxscore option removed; computed property instead</p> <p>Parameters:</p> Name Type Description Default <code>searcher</code> <p>IndexSearcher which can retrieve documents</p> required <code>scoredocs</code> <code>Sequence</code> <p>lucene ScoreDocs</p> required <code>count</code> <p>total number of hits; float indicates estimate</p> <code>0</code> <code>fields</code> <p>optional field selectors</p> <code>None</code> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class Hits:\n    \"\"\"Search results: lazily evaluated and memory efficient.\n\n    Provides a read-only sequence interface to hit objects.\n\n    Note:\n        changed in version 2.3: maxscore option removed; computed property instead\n\n    Args:\n        searcher: [IndexSearcher][lupyne.engine.indexers.IndexSearcher] which can retrieve documents\n        scoredocs: lucene ScoreDocs\n        count: total number of hits; float indicates estimate\n        fields: optional field selectors\n    \"\"\"\n\n    def __init__(self, searcher, scoredocs: Sequence, count=0, fields=None):\n        self.searcher, self.scoredocs = searcher, scoredocs\n        if hasattr(count, 'relation'):\n            cls = int if count.relation == search.TotalHits.Relation.EQUAL_TO else float\n            count = cls(count.value())\n        self.count, self.fields = count, fields\n\n    def select(self, *fields: str):\n        \"\"\"Only load selected fields.\"\"\"\n        self.fields = HashSet(Arrays.asList(fields))\n\n    def __len__(self):\n        return len(self.scoredocs)\n\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            scoredocs = list(map(self.scoredocs.__getitem__, range(*index.indices(len(self)))))\n            return type(self)(self.searcher, scoredocs, self.count, self.fields)\n        scoredoc = self.scoredocs[index]\n        keys = search.FieldDoc.cast_(scoredoc).fields if search.FieldDoc.instance_(scoredoc) else ()\n        storedFields = self.searcher.storedFields()\n        doc = storedFields.document(scoredoc.doc, *([self.fields] * (self.fields is not None)))\n        return Hit(doc, scoredoc.doc, scoredoc.score, keys)\n\n    @property\n    def ids(self) -&gt; Iterator[int]:\n        return map(operator.attrgetter('doc'), self.scoredocs)\n\n    @property\n    def scores(self) -&gt; Iterator[float]:\n        return map(operator.attrgetter('score'), self.scoredocs)\n\n    @property\n    def maxscore(self) -&gt; float:\n        \"\"\"max score of present hits; not necessarily of all matches\"\"\"\n        return max(self.scores, default=float('nan'))\n\n    def items(self) -&gt; Iterator[tuple]:\n        \"\"\"Generate zipped ids and scores.\"\"\"\n        return map(operator.attrgetter('doc', 'score'), self.scoredocs)\n\n    def highlights(self, query: search.Query, **fields: int) -&gt; Iterator[dict]:\n        \"\"\"Generate highlighted fields for each hit.\n\n        Args:\n            query: lucene Query\n            **fields: mapping of fields to maxinum number of passages\n        \"\"\"\n        mapping = self.searcher.highlighter.highlightFields(\n            list(fields), query, list(self.ids), list(fields.values())\n        )\n        mapping = {field: lucene.JArray_string.cast_(mapping.get(field)) for field in fields}\n        return (dict(zip(mapping, values)) for values in zip(*mapping.values()))\n\n    def docvalues(self, field: str, type=None) -&gt; dict:\n        \"\"\"Return mapping of docs to docvalues.\"\"\"\n        return self.searcher.docvalues(field, type).select(self.ids)\n\n    def groupby(\n        self, func: Callable, count: int | None = None, docs: int | None = None\n    ) -&gt; 'Groups':\n        \"\"\"Return ordered list of [Hits][lupyne.engine.documents.Hits] grouped by value of function applied to doc ids.\n\n        Optionally limit the number of groups and docs per group.\n        \"\"\"\n        groups: dict = collections.OrderedDict()\n        for scoredoc in self.scoredocs:\n            value = func(scoredoc.doc)\n            try:\n                group = groups[value]\n            except KeyError:\n                group = groups[value] = type(self)(self.searcher, [], fields=self.fields)\n                group.value = value  # type: ignore\n            group.scoredocs.append(scoredoc)\n        groups = list(groups.values())  # type: ignore\n        for group in groups:\n            group.count = len(group)\n            group.scoredocs = group.scoredocs[:docs]\n        return Groups(self.searcher, groups[:count], len(groups), self.fields)\n\n    def filter(self, func: Callable) -&gt; 'Hits':\n        \"\"\"Return [Hits][lupyne.engine.documents.Hits] filtered by function applied to doc ids.\"\"\"\n        scoredocs = [scoredoc for scoredoc in self.scoredocs if func(scoredoc.doc)]\n        return type(self)(self.searcher, scoredocs, fields=self.fields)\n\n    def sorted(self, key: Callable, reverse=False) -&gt; 'Hits':\n        \"\"\"Return [Hits][lupyne.engine.documents.Hits] sorted by key function applied to doc ids.\"\"\"\n        scoredocs = sorted(self.scoredocs, key=lambda scoredoc: key(scoredoc.doc), reverse=reverse)\n        return type(self)(self.searcher, scoredocs, self.count, self.fields)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits.maxscore","title":"<code>maxscore</code>  <code>property</code>","text":"<p>max score of present hits; not necessarily of all matches</p>"},{"location":"engine/#lupyne.engine.documents.Hits.docvalues","title":"<code>docvalues(field, type=None)</code>","text":"<p>Return mapping of docs to docvalues.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def docvalues(self, field: str, type=None) -&gt; dict:\n    \"\"\"Return mapping of docs to docvalues.\"\"\"\n    return self.searcher.docvalues(field, type).select(self.ids)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits.filter","title":"<code>filter(func)</code>","text":"<p>Return Hits filtered by function applied to doc ids.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def filter(self, func: Callable) -&gt; 'Hits':\n    \"\"\"Return [Hits][lupyne.engine.documents.Hits] filtered by function applied to doc ids.\"\"\"\n    scoredocs = [scoredoc for scoredoc in self.scoredocs if func(scoredoc.doc)]\n    return type(self)(self.searcher, scoredocs, fields=self.fields)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits.groupby","title":"<code>groupby(func, count=None, docs=None)</code>","text":"<p>Return ordered list of Hits grouped by value of function applied to doc ids.</p> <p>Optionally limit the number of groups and docs per group.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def groupby(\n    self, func: Callable, count: int | None = None, docs: int | None = None\n) -&gt; 'Groups':\n    \"\"\"Return ordered list of [Hits][lupyne.engine.documents.Hits] grouped by value of function applied to doc ids.\n\n    Optionally limit the number of groups and docs per group.\n    \"\"\"\n    groups: dict = collections.OrderedDict()\n    for scoredoc in self.scoredocs:\n        value = func(scoredoc.doc)\n        try:\n            group = groups[value]\n        except KeyError:\n            group = groups[value] = type(self)(self.searcher, [], fields=self.fields)\n            group.value = value  # type: ignore\n        group.scoredocs.append(scoredoc)\n    groups = list(groups.values())  # type: ignore\n    for group in groups:\n        group.count = len(group)\n        group.scoredocs = group.scoredocs[:docs]\n    return Groups(self.searcher, groups[:count], len(groups), self.fields)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits.highlights","title":"<code>highlights(query, **fields)</code>","text":"<p>Generate highlighted fields for each hit.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Query</code> <p>lucene Query</p> required <code>**fields</code> <code>int</code> <p>mapping of fields to maxinum number of passages</p> <code>{}</code> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def highlights(self, query: search.Query, **fields: int) -&gt; Iterator[dict]:\n    \"\"\"Generate highlighted fields for each hit.\n\n    Args:\n        query: lucene Query\n        **fields: mapping of fields to maxinum number of passages\n    \"\"\"\n    mapping = self.searcher.highlighter.highlightFields(\n        list(fields), query, list(self.ids), list(fields.values())\n    )\n    mapping = {field: lucene.JArray_string.cast_(mapping.get(field)) for field in fields}\n    return (dict(zip(mapping, values)) for values in zip(*mapping.values()))\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits.items","title":"<code>items()</code>","text":"<p>Generate zipped ids and scores.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def items(self) -&gt; Iterator[tuple]:\n    \"\"\"Generate zipped ids and scores.\"\"\"\n    return map(operator.attrgetter('doc', 'score'), self.scoredocs)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits.select","title":"<code>select(*fields)</code>","text":"<p>Only load selected fields.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def select(self, *fields: str):\n    \"\"\"Only load selected fields.\"\"\"\n    self.fields = HashSet(Arrays.asList(fields))\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Hits.sorted","title":"<code>sorted(key, reverse=False)</code>","text":"<p>Return Hits sorted by key function applied to doc ids.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def sorted(self, key: Callable, reverse=False) -&gt; 'Hits':\n    \"\"\"Return [Hits][lupyne.engine.documents.Hits] sorted by key function applied to doc ids.\"\"\"\n    scoredocs = sorted(self.scoredocs, key=lambda scoredoc: key(scoredoc.doc), reverse=reverse)\n    return type(self)(self.searcher, scoredocs, self.count, self.fields)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Groups","title":"<code>lupyne.engine.documents.Groups</code>","text":"<p>Sequence of grouped Hits.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class Groups:\n    \"\"\"Sequence of grouped [Hits][lupyne.engine.documents.Hits].\"\"\"\n\n    select = Hits.select\n\n    def __init__(self, searcher, groupdocs: Sequence, count: int = 0, fields=None):\n        self.searcher, self.groupdocs = searcher, groupdocs\n        self.count, self.fields = count, fields\n\n    def __len__(self):\n        return len(self.groupdocs)\n\n    def __getitem__(self, index):\n        hits = groupdocs = self.groupdocs[index]\n        if isinstance(groupdocs, grouping.GroupDocs):\n            hits = Hits(self.searcher, groupdocs.scoreDocs(), groupdocs.totalHits())\n            hits.value = convert(groupdocs.groupValue())\n        hits.fields = self.fields\n        return hits\n\n    @property\n    def facets(self):\n        \"\"\"mapping of field values and counts\"\"\"\n        return {hits.value: hits.count for hits in self}\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Groups.facets","title":"<code>facets</code>  <code>property</code>","text":"<p>mapping of field values and counts</p>"},{"location":"engine/#lupyne.engine.documents.GroupingSearch","title":"<code>lupyne.engine.documents.GroupingSearch</code>","text":"<p>               Bases: <code>GroupingSearch</code></p> <p>Inherited lucene GroupingSearch with optimized faceting.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>unique field name to group by</p> required <code>sort</code> <p>lucene Sort to order groups and docs</p> <code>None</code> <code>cache</code> <p>use unlimited caching</p> <code>True</code> <code>**attrs</code> <p>additional attributes to set</p> <code>{}</code> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class GroupingSearch(grouping.GroupingSearch):\n    \"\"\"Inherited lucene GroupingSearch with optimized faceting.\n\n    Args:\n        field: unique field name to group by\n        sort: lucene Sort to order groups and docs\n        cache: use unlimited caching\n        **attrs: additional attributes to set\n    \"\"\"\n\n    def __init__(self, field: str, sort=None, cache=True, **attrs):\n        super().__init__(field)\n        self.field = field\n        if sort:\n            self.groupSort = self.sortWithinGroup = sort\n            self.fillSortFields = True\n        if cache:\n            self.setCachingInMB(float('inf'), True)\n        for name in attrs:\n            getattr(type(self), name).__set__(self, attrs[name])\n\n    def __len__(self):\n        return self.allMatchingGroups.size()\n\n    def __iter__(self):\n        return map(convert, self.allMatchingGroups)\n\n    def search(\n        self, searcher, query: search.Query, count: int | None = None, start: int = 0\n    ) -&gt; Groups:\n        \"\"\"Run query and return [Groups][lupyne.engine.documents.Groups].\"\"\"\n        if count is None:\n            count = sum(\n                index.DocValues.getSorted(reader, self.field).valueCount\n                for reader in searcher.readers\n            )\n        topgroups = super().search(searcher, query, start, max(count - start, 1))\n        return Groups(searcher, topgroups.groups, topgroups.totalHitCount)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.GroupingSearch.search","title":"<code>search(searcher, query, count=None, start=0)</code>","text":"<p>Run query and return Groups.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def search(\n    self, searcher, query: search.Query, count: int | None = None, start: int = 0\n) -&gt; Groups:\n    \"\"\"Run query and return [Groups][lupyne.engine.documents.Groups].\"\"\"\n    if count is None:\n        count = sum(\n            index.DocValues.getSorted(reader, self.field).valueCount\n            for reader in searcher.readers\n        )\n    topgroups = super().search(searcher, query, start, max(count - start, 1))\n    return Groups(searcher, topgroups.groups, topgroups.totalHitCount)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Field","title":"<code>lupyne.engine.documents.Field</code>","text":"<p>               Bases: <code>FieldType</code></p> <p>Saved parameters which can generate lucene Fields given values.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of field</p> required Source code in <code>lupyne/engine/documents.py</code> <pre><code>class Field(FieldType):  # type: ignore\n    \"\"\"Saved parameters which can generate lucene Fields given values.\n\n    Args:\n        name: name of field\n    \"\"\"\n\n    docValuesType = property(FieldType.docValuesType, FieldType.setDocValuesType)\n    indexOptions = property(FieldType.indexOptions, FieldType.setIndexOptions)\n    omitNorms = property(FieldType.omitNorms, FieldType.setOmitNorms)\n    stored = property(FieldType.stored, FieldType.setStored)\n    storeTermVectorOffsets = property(\n        FieldType.storeTermVectorOffsets, FieldType.setStoreTermVectorOffsets\n    )\n    storeTermVectorPayloads = property(\n        FieldType.storeTermVectorPayloads, FieldType.setStoreTermVectorPayloads\n    )\n    storeTermVectorPositions = property(\n        FieldType.storeTermVectorPositions, FieldType.setStoreTermVectorPositions\n    )\n    storeTermVectors = property(FieldType.storeTermVectors, FieldType.setStoreTermVectors)\n    tokenized = property(FieldType.tokenized, FieldType.setTokenized)\n\n    properties = {name for name in locals() if not name.startswith('__')}\n    types = {int: 'long', float: 'double', str: 'string'}\n    types.update(  # type: ignore\n        NUMERIC='long', BINARY='string', SORTED='string', SORTED_NUMERIC='long', SORTED_SET='string'\n    )\n    dimensions = property(\n        FieldType.pointDimensionCount,\n        lambda self, count: self.setDimensions(count, Long.BYTES),\n    )\n\n    def __init__(self, name: str, docValuesType='', indexOptions='', dimensions=0, **settings):\n        super().__init__()\n        self.name = name\n        for name in self.properties.intersection(settings):\n            setattr(self, name, settings.pop(name))\n        for name in settings:\n            raise AttributeError(f\"'Field' object has no property '{name}'\")\n        if dimensions:\n            self.dimensions = dimensions\n        if indexOptions:\n            self.indexOptions = getattr(index.IndexOptions, indexOptions.upper())\n        if docValuesType:\n            self.docValuesType = getattr(index.DocValuesType, docValuesType.upper())\n            name = docValuesType.title().replace('_', '')\n            self.docValueClass = getattr(document, name + 'DocValuesField')\n            if self.stored or self.indexed or self.dimensions:\n                settings = self.settings\n                del settings['docValuesType']\n                self.docValueLess = Field(self.name, **settings)\n        assert self.stored or self.indexed or self.docvalues or self.dimensions\n\n    @classmethod\n    def String(\n        cls, name: str, tokenized=False, omitNorms=True, indexOptions='DOCS', **settings\n    ) -&gt; 'Field':\n        \"\"\"Return Field with default settings for strings.\"\"\"\n        settings.update(tokenized=tokenized, omitNorms=omitNorms, indexOptions=indexOptions)\n        return cls(name, **settings)\n\n    @classmethod\n    def Text(cls, name: str, indexOptions='DOCS_AND_FREQS_AND_POSITIONS', **settings) -&gt; 'Field':\n        \"\"\"Return Field with default settings for text.\"\"\"\n        return cls(name, indexOptions=indexOptions, **settings)\n\n    @property\n    def indexed(self):\n        return self.indexOptions != index.IndexOptions.NONE\n\n    @property\n    def docvalues(self):\n        return self.docValuesType != index.DocValuesType.NONE\n\n    @property\n    def settings(self) -&gt; dict:\n        \"\"\"dict representation of settings\"\"\"\n        defaults = FieldType()\n        result = {'dimensions': self.dimensions} if self.dimensions else {}\n        for name in Field.properties:\n            value = getattr(self, name)\n            if value != getattr(defaults, name)():\n                result[name] = value if isinstance(value, int) else str(value)\n        return result\n\n    def items(self, *values) -&gt; Iterator[document.Field]:\n        \"\"\"Generate lucene Fields suitable for adding to a document.\"\"\"\n        if self.docvalues:\n            types = {int: int, float: util.NumericUtils.doubleToSortableLong}\n            for value in values:\n                yield self.docValueClass(self.name, types.get(type(value), util.BytesRef)(value))\n            self = getattr(self, 'docValueLess', self)  # type: ignore\n        if self.dimensions:\n            for value in values:\n                cls = document.LongPoint if isinstance(value, int) else document.DoublePoint\n                yield cls(self.name, value)\n        if self.indexed:\n            for value in values:\n                yield document.Field(self.name, value, self)\n        elif self.stored:\n            for value in values:\n                yield document.StoredField(self.name, value)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Field.settings","title":"<code>settings</code>  <code>property</code>","text":"<p>dict representation of settings</p>"},{"location":"engine/#lupyne.engine.documents.Field.String","title":"<code>String(name, tokenized=False, omitNorms=True, indexOptions='DOCS', **settings)</code>  <code>classmethod</code>","text":"<p>Return Field with default settings for strings.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>@classmethod\ndef String(\n    cls, name: str, tokenized=False, omitNorms=True, indexOptions='DOCS', **settings\n) -&gt; 'Field':\n    \"\"\"Return Field with default settings for strings.\"\"\"\n    settings.update(tokenized=tokenized, omitNorms=omitNorms, indexOptions=indexOptions)\n    return cls(name, **settings)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Field.Text","title":"<code>Text(name, indexOptions='DOCS_AND_FREQS_AND_POSITIONS', **settings)</code>  <code>classmethod</code>","text":"<p>Return Field with default settings for text.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>@classmethod\ndef Text(cls, name: str, indexOptions='DOCS_AND_FREQS_AND_POSITIONS', **settings) -&gt; 'Field':\n    \"\"\"Return Field with default settings for text.\"\"\"\n    return cls(name, indexOptions=indexOptions, **settings)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.Field.items","title":"<code>items(*values)</code>","text":"<p>Generate lucene Fields suitable for adding to a document.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def items(self, *values) -&gt; Iterator[document.Field]:\n    \"\"\"Generate lucene Fields suitable for adding to a document.\"\"\"\n    if self.docvalues:\n        types = {int: int, float: util.NumericUtils.doubleToSortableLong}\n        for value in values:\n            yield self.docValueClass(self.name, types.get(type(value), util.BytesRef)(value))\n        self = getattr(self, 'docValueLess', self)  # type: ignore\n    if self.dimensions:\n        for value in values:\n            cls = document.LongPoint if isinstance(value, int) else document.DoublePoint\n            yield cls(self.name, value)\n    if self.indexed:\n        for value in values:\n            yield document.Field(self.name, value, self)\n    elif self.stored:\n        for value in values:\n            yield document.StoredField(self.name, value)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.NestedField","title":"<code>lupyne.engine.documents.NestedField</code>","text":"<p>               Bases: <code>Field</code></p> <p>Field which indexes every component into its own field.</p> <p>Original value may be stored for convenience.</p> <p>Parameters:</p> Name Type Description Default <code>sep</code> <code>str</code> <p>field separator used on name and values</p> <code>'.'</code> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class NestedField(Field):\n    \"\"\"Field which indexes every component into its own field.\n\n    Original value may be stored for convenience.\n\n    Args:\n        sep: field separator used on name and values\n    \"\"\"\n\n    def __init__(self, name: str, sep: str = '.', **settings):\n        super().__init__(name, **Field.String(name, **settings).settings)\n        self.sep = sep\n        self.names = tuple(self.values(name))\n\n    def values(self, value: str) -&gt; Iterator[str]:\n        \"\"\"Generate component field values in order.\"\"\"\n        values = value.split(self.sep)\n        for stop in range(1, len(values) + 1):\n            yield self.sep.join(values[:stop])\n\n    def items(self, *values: str) -&gt; Iterator[document.Field]:\n        \"\"\"Generate indexed component fields.\"\"\"\n        field = getattr(self, 'docValueLess', self)\n        for value in values:\n            for name, text in zip(self.names, self.values(value)):\n                yield document.Field(name, text, field)\n                if self.docvalues:\n                    yield self.docValueClass(name, util.BytesRef(text))\n\n    def prefix(self, value: str) -&gt; Query:\n        \"\"\"Return prefix query of the closest possible prefixed field.\"\"\"\n        index = value.count(self.sep)\n        return Query.prefix(self.names[index], value)\n\n    def range(self, start, stop, lower=True, upper=False) -&gt; Query:\n        \"\"\"Return range query of the closest possible prefixed field.\"\"\"\n        index = max(value.count(self.sep) for value in (start, stop) if value is not None)\n        return Query.range(self.names[index], start, stop, lower, upper)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.NestedField.items","title":"<code>items(*values)</code>","text":"<p>Generate indexed component fields.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def items(self, *values: str) -&gt; Iterator[document.Field]:\n    \"\"\"Generate indexed component fields.\"\"\"\n    field = getattr(self, 'docValueLess', self)\n    for value in values:\n        for name, text in zip(self.names, self.values(value)):\n            yield document.Field(name, text, field)\n            if self.docvalues:\n                yield self.docValueClass(name, util.BytesRef(text))\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.NestedField.prefix","title":"<code>prefix(value)</code>","text":"<p>Return prefix query of the closest possible prefixed field.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def prefix(self, value: str) -&gt; Query:\n    \"\"\"Return prefix query of the closest possible prefixed field.\"\"\"\n    index = value.count(self.sep)\n    return Query.prefix(self.names[index], value)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.NestedField.range","title":"<code>range(start, stop, lower=True, upper=False)</code>","text":"<p>Return range query of the closest possible prefixed field.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def range(self, start, stop, lower=True, upper=False) -&gt; Query:\n    \"\"\"Return range query of the closest possible prefixed field.\"\"\"\n    index = max(value.count(self.sep) for value in (start, stop) if value is not None)\n    return Query.range(self.names[index], start, stop, lower, upper)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.NestedField.values","title":"<code>values(value)</code>","text":"<p>Generate component field values in order.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def values(self, value: str) -&gt; Iterator[str]:\n    \"\"\"Generate component field values in order.\"\"\"\n    values = value.split(self.sep)\n    for stop in range(1, len(values) + 1):\n        yield self.sep.join(values[:stop])\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.DateTimeField","title":"<code>lupyne.engine.documents.DateTimeField</code>","text":"<p>               Bases: <code>Field</code></p> <p>Field which indexes datetimes as Point fields of timestamps.</p> <p>Supports datetimes, dates, and any prefix of time tuples.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class DateTimeField(Field):\n    \"\"\"Field which indexes datetimes as Point fields of timestamps.\n\n    Supports datetimes, dates, and any prefix of time tuples.\n    \"\"\"\n\n    def __init__(self, name: str, dimensions: int = 1, **settings):\n        super().__init__(name, dimensions=dimensions, **settings)\n\n    @classmethod\n    def timestamp(cls, date) -&gt; float:\n        \"\"\"Return utc timestamp from date or time tuple.\"\"\"\n        if isinstance(date, datetime.date):\n            return calendar.timegm(date.timetuple()) + getattr(date, 'microsecond', 0) * 1e-6\n        return float(calendar.timegm(tuple(date) + (None, 1, 1, 0, 0, 0)[len(date) :]))\n\n    def items(self, *dates) -&gt; Iterator[document.Field]:\n        \"\"\"Generate lucene NumericFields of timestamps.\"\"\"\n        return super().items(*map(self.timestamp, dates))\n\n    def range(self, start, stop, **inclusive) -&gt; Query:\n        \"\"\"Return NumericRangeQuery of timestamps.\"\"\"\n        interval = (date and self.timestamp(date) for date in (start, stop))\n        return Query.ranges(self.name, interval, **inclusive)\n\n    def prefix(self, date) -&gt; Query:\n        \"\"\"Return range query which matches the date prefix.\"\"\"\n        if isinstance(date, datetime.date):\n            date = date.timetuple()[: 6 if isinstance(date, datetime.datetime) else 3]\n        if len(date) == 2 and date[1] == 12:  # month must be valid\n            return self.range(date, (date[0] + 1, 1))\n        return self.range(date, tuple(date[:-1]) + (date[-1] + 1,))\n\n    def duration(self, date, days=0, **delta) -&gt; Query:\n        \"\"\"Return date range query within time span of date.\n\n        Args:\n            date: origin date or tuple\n            days **delta:: timedelta parameters\n        \"\"\"\n        if not isinstance(date, datetime.date):\n            date = datetime.datetime(*(tuple(date) + (None, 1, 1)[len(date) :]))\n        delta = datetime.timedelta(days, **delta)  # type: ignore\n        return self.range(*sorted([date, date + delta]), upper=True)\n\n    def within(self, days=0, weeks=0, tz=None, **delta) -&gt; Query:\n        \"\"\"Return date range query within current time and delta.\n\n        If the delta is an exact number of days, then dates will be used.\n\n        Args:\n            days weeks: number of days to offset from today\n            tz: optional timezone\n            **delta: additional timedelta parameters\n        \"\"\"\n        date = datetime.datetime.now(tz)\n        if not (isinstance(days + weeks, float) or delta):\n            date = date.date()  # type: ignore\n        return self.duration(date, days, weeks=weeks, **delta)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.DateTimeField.duration","title":"<code>duration(date, days=0, **delta)</code>","text":"<p>Return date range query within time span of date.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <p>origin date or tuple</p> required <code>days **delta</code> <p>: timedelta parameters</p> required Source code in <code>lupyne/engine/documents.py</code> <pre><code>def duration(self, date, days=0, **delta) -&gt; Query:\n    \"\"\"Return date range query within time span of date.\n\n    Args:\n        date: origin date or tuple\n        days **delta:: timedelta parameters\n    \"\"\"\n    if not isinstance(date, datetime.date):\n        date = datetime.datetime(*(tuple(date) + (None, 1, 1)[len(date) :]))\n    delta = datetime.timedelta(days, **delta)  # type: ignore\n    return self.range(*sorted([date, date + delta]), upper=True)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.DateTimeField.items","title":"<code>items(*dates)</code>","text":"<p>Generate lucene NumericFields of timestamps.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def items(self, *dates) -&gt; Iterator[document.Field]:\n    \"\"\"Generate lucene NumericFields of timestamps.\"\"\"\n    return super().items(*map(self.timestamp, dates))\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.DateTimeField.prefix","title":"<code>prefix(date)</code>","text":"<p>Return range query which matches the date prefix.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def prefix(self, date) -&gt; Query:\n    \"\"\"Return range query which matches the date prefix.\"\"\"\n    if isinstance(date, datetime.date):\n        date = date.timetuple()[: 6 if isinstance(date, datetime.datetime) else 3]\n    if len(date) == 2 and date[1] == 12:  # month must be valid\n        return self.range(date, (date[0] + 1, 1))\n    return self.range(date, tuple(date[:-1]) + (date[-1] + 1,))\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.DateTimeField.range","title":"<code>range(start, stop, **inclusive)</code>","text":"<p>Return NumericRangeQuery of timestamps.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def range(self, start, stop, **inclusive) -&gt; Query:\n    \"\"\"Return NumericRangeQuery of timestamps.\"\"\"\n    interval = (date and self.timestamp(date) for date in (start, stop))\n    return Query.ranges(self.name, interval, **inclusive)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.DateTimeField.timestamp","title":"<code>timestamp(date)</code>  <code>classmethod</code>","text":"<p>Return utc timestamp from date or time tuple.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>@classmethod\ndef timestamp(cls, date) -&gt; float:\n    \"\"\"Return utc timestamp from date or time tuple.\"\"\"\n    if isinstance(date, datetime.date):\n        return calendar.timegm(date.timetuple()) + getattr(date, 'microsecond', 0) * 1e-6\n    return float(calendar.timegm(tuple(date) + (None, 1, 1, 0, 0, 0)[len(date) :]))\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.DateTimeField.within","title":"<code>within(days=0, weeks=0, tz=None, **delta)</code>","text":"<p>Return date range query within current time and delta.</p> <p>If the delta is an exact number of days, then dates will be used.</p> <p>Parameters:</p> Name Type Description Default <code>days weeks</code> <p>number of days to offset from today</p> required <code>tz</code> <p>optional timezone</p> <code>None</code> <code>**delta</code> <p>additional timedelta parameters</p> <code>{}</code> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def within(self, days=0, weeks=0, tz=None, **delta) -&gt; Query:\n    \"\"\"Return date range query within current time and delta.\n\n    If the delta is an exact number of days, then dates will be used.\n\n    Args:\n        days weeks: number of days to offset from today\n        tz: optional timezone\n        **delta: additional timedelta parameters\n    \"\"\"\n    date = datetime.datetime.now(tz)\n    if not (isinstance(days + weeks, float) or delta):\n        date = date.date()  # type: ignore\n    return self.duration(date, days, weeks=weeks, **delta)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.ShapeField","title":"<code>lupyne.engine.documents.ShapeField</code>","text":"<p>Field which indexes geometries: LatLon or XY.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>class ShapeField:\n    \"\"\"Field which indexes geometries: LatLon or XY.\"\"\"\n\n    def __init__(self, name: str, indexed=True, docvalues=False):\n        self.name, self.indexed, self.docvalues = name, bool(indexed), bool(docvalues)\n\n    def apply(self, func: Callable, shape: geo.Geometry):\n        if isinstance(shape, geo.Point):\n            return func(self.name, shape.lat, shape.lon)\n        if isinstance(shape, geo.XYPoint):\n            return func(self.name, shape.x, shape.y)\n        return func(self.name, shape)\n\n    def items(self, *shapes: geo.Geometry) -&gt; Iterator[document.Field]:\n        \"\"\"Generate lucene shape fields from geometries.\"\"\"\n        for shape in shapes:\n            cls = document.XYShape if isinstance(shape, geo.XYGeometry) else document.LatLonShape\n            if self.indexed:\n                yield from self.apply(cls.createIndexableFields, shape)\n            if self.docvalues:\n                yield self.apply(cls.createDocValueField, shape)\n\n    def distances(self, point: geo.Point | geo.XYPoint) -&gt; search.SortField:\n        \"\"\"Return distance SortField.\"\"\"\n        xy = isinstance(point, geo.XYGeometry)\n        cls = document.XYDocValuesField if xy else document.LatLonDocValuesField\n        return self.apply(cls.newDistanceSort, point)\n\n    def query(self, relation: QueryRelation, *shapes: geo.Geometry) -&gt; search.Query:  # type: ignore\n        shape = shapes[0]\n        cls = document.XYShape if isinstance(shape, geo.XYGeometry) else document.LatLonShape\n        func = cls.newGeometryQuery\n        if isinstance(shape, (geo.Line, geo.XYLine)):\n            func = cls.newLineQuery\n        if isinstance(shape, (geo.Circle, geo.XYCircle)):\n            func = cls.newDistanceQuery\n        if isinstance(shape, (geo.Polygon, geo.XYPolygon)):\n            func = cls.newPolygonQuery\n        return func(self.name, relation, *shapes)\n\n    def contains(self, *shapes: geo.Geometry) -&gt; search.Query:\n        \"\"\"Return shape query with `contains` relation.\"\"\"\n        return self.query(QueryRelation.CONTAINS, *shapes)\n\n    def disjoint(self, *shapes: geo.Geometry) -&gt; search.Query:\n        \"\"\"Return shape query with `disjoint` relation.\"\"\"\n        return self.query(QueryRelation.DISJOINT, *shapes)\n\n    def intersects(self, *shapes: geo.Geometry) -&gt; search.Query:\n        \"\"\"Return shape query with `intersects` relation.\"\"\"\n        return self.query(QueryRelation.INTERSECTS, *shapes)\n\n    def within(self, *shapes: geo.Geometry) -&gt; search.Query:\n        \"\"\"Return shape query with `within` relation.\"\"\"\n        return self.query(QueryRelation.WITHIN, *shapes)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.ShapeField.contains","title":"<code>contains(*shapes)</code>","text":"<p>Return shape query with <code>contains</code> relation.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def contains(self, *shapes: geo.Geometry) -&gt; search.Query:\n    \"\"\"Return shape query with `contains` relation.\"\"\"\n    return self.query(QueryRelation.CONTAINS, *shapes)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.ShapeField.disjoint","title":"<code>disjoint(*shapes)</code>","text":"<p>Return shape query with <code>disjoint</code> relation.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def disjoint(self, *shapes: geo.Geometry) -&gt; search.Query:\n    \"\"\"Return shape query with `disjoint` relation.\"\"\"\n    return self.query(QueryRelation.DISJOINT, *shapes)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.ShapeField.distances","title":"<code>distances(point)</code>","text":"<p>Return distance SortField.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def distances(self, point: geo.Point | geo.XYPoint) -&gt; search.SortField:\n    \"\"\"Return distance SortField.\"\"\"\n    xy = isinstance(point, geo.XYGeometry)\n    cls = document.XYDocValuesField if xy else document.LatLonDocValuesField\n    return self.apply(cls.newDistanceSort, point)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.ShapeField.intersects","title":"<code>intersects(*shapes)</code>","text":"<p>Return shape query with <code>intersects</code> relation.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def intersects(self, *shapes: geo.Geometry) -&gt; search.Query:\n    \"\"\"Return shape query with `intersects` relation.\"\"\"\n    return self.query(QueryRelation.INTERSECTS, *shapes)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.ShapeField.items","title":"<code>items(*shapes)</code>","text":"<p>Generate lucene shape fields from geometries.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def items(self, *shapes: geo.Geometry) -&gt; Iterator[document.Field]:\n    \"\"\"Generate lucene shape fields from geometries.\"\"\"\n    for shape in shapes:\n        cls = document.XYShape if isinstance(shape, geo.XYGeometry) else document.LatLonShape\n        if self.indexed:\n            yield from self.apply(cls.createIndexableFields, shape)\n        if self.docvalues:\n            yield self.apply(cls.createDocValueField, shape)\n</code></pre>"},{"location":"engine/#lupyne.engine.documents.ShapeField.within","title":"<code>within(*shapes)</code>","text":"<p>Return shape query with <code>within</code> relation.</p> Source code in <code>lupyne/engine/documents.py</code> <pre><code>def within(self, *shapes: geo.Geometry) -&gt; search.Query:\n    \"\"\"Return shape query with `within` relation.\"\"\"\n    return self.query(QueryRelation.WITHIN, *shapes)\n</code></pre>"},{"location":"engine/#queries","title":"queries","text":""},{"location":"engine/#lupyne.engine.queries.Query","title":"<code>lupyne.engine.queries.Query</code>","text":"<p>Inherited lucene Query, with dynamic base class acquisition.</p> <p>Uses class methods and operator overloading for convenient query construction.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>class Query:\n    \"\"\"Inherited lucene Query, with dynamic base class acquisition.\n\n    Uses class methods and operator overloading for convenient query construction.\n    \"\"\"\n\n    def __new__(cls, base, *args):\n        return base.__new__(type(base.__name__, (cls, base), {}))\n\n    def __init__(self, base: search.Query, *args):\n        base.__init__(self, *args)\n\n    @classmethod\n    def term(cls, name: str, value) -&gt; 'Query':\n        \"\"\"Return lucene TermQuery.\"\"\"\n        return cls(search.TermQuery, index.Term(name, value))\n\n    @classmethod\n    def terms(cls, name: str, values) -&gt; 'Query':\n        \"\"\"Return lucene TermInSetQuery, optimizing a SHOULD BooleanQuery of many terms.\"\"\"\n        return cls(search.TermInSetQuery, name, Arrays.asList(list(map(util.BytesRef, values))))\n\n    @classmethod\n    def boolean(cls, occur, *queries, **terms):\n        builder = search.BooleanQuery.Builder()\n        for query in queries:\n            builder.add(query, occur)\n        for name, values in terms.items():\n            for value in [values] if isinstance(values, str) else values:\n                builder.add(cls.term(name, value), occur)\n        return builder.build()\n\n    @classmethod\n    def any(cls, *queries: search.Query, **terms) -&gt; search.BooleanQuery:\n        \"\"\"Return lucene BooleanQuery with SHOULD clauses from queries and terms.\"\"\"\n        return cls.boolean(search.BooleanClause.Occur.SHOULD, *queries, **terms)\n\n    @classmethod\n    def all(cls, *queries: search.Query, **terms) -&gt; search.BooleanQuery:\n        \"\"\"Return lucene BooleanQuery with MUST clauses from queries and terms.\"\"\"\n        return cls.boolean(search.BooleanClause.Occur.MUST, *queries, **terms)\n\n    @classmethod\n    def filter(cls, *queries: search.Query, **terms) -&gt; search.BooleanQuery:\n        \"\"\"Return lucene BooleanQuery with FILTER clauses from queries and terms.\"\"\"\n        return cls.boolean(search.BooleanClause.Occur.FILTER, *queries, **terms)\n\n    @classmethod\n    def disjunct(cls, multiplier, *queries, **terms):\n        \"\"\"Return lucene DisjunctionMaxQuery from queries and terms.\"\"\"\n        queries = list(queries)\n        for name, values in terms.items():\n            if isinstance(values, str):\n                values = [values]\n            queries += (cls.term(name, value) for value in values)\n        return cls(search.DisjunctionMaxQuery, Arrays.asList(queries), multiplier)\n\n    @classmethod\n    def span(cls, *term) -&gt; 'SpanQuery':\n        \"\"\"Return [SpanQuery][lupyne.engine.queries.SpanQuery] from term name and value or a MultiTermQuery.\"\"\"\n        if len(term) &lt;= 1:\n            return SpanQuery(spans.SpanMultiTermQueryWrapper, *term)\n        return SpanQuery(spans.SpanTermQuery, index.Term(*term))\n\n    @classmethod\n    def near(cls, name: str, *values, **kwargs) -&gt; 'SpanQuery':\n        \"\"\"Return [SpanNearQuery][lupyne.engine.queries.SpanQuery.near] from terms.\n        Term values which supply another field name will be masked.\"\"\"\n        spans = (\n            cls.span(name, value) if isinstance(value, str) else cls.span(*value).mask(name)\n            for value in values\n        )\n        return SpanQuery.near(*spans, **kwargs)\n\n    @classmethod\n    def prefix(cls, name: str, value) -&gt; 'Query':\n        \"\"\"Return lucene PrefixQuery.\"\"\"\n        return cls(search.PrefixQuery, index.Term(name, value))\n\n    @classmethod\n    def range(cls, name: str, start, stop, lower=True, upper=False) -&gt; 'Query':\n        \"\"\"Return lucene RangeQuery, by default with a half-open interval.\"\"\"\n        start, stop = (value if value is None else util.BytesRef(value) for value in (start, stop))\n        return cls(search.TermRangeQuery, name, start, stop, lower, upper)\n\n    @classmethod\n    def phrase(cls, name: str, *values, **attrs) -&gt; search.MultiPhraseQuery:\n        \"\"\"Return lucene MultiPhraseQuery.  None may be used as a placeholder.\"\"\"\n        builder = search.MultiPhraseQuery.Builder()\n        for attr in attrs:\n            setattr(builder, attr, attrs[attr])\n        for idx, words in enumerate(values):\n            if isinstance(words, str):\n                words = [words]\n            if words is not None:\n                builder.add([index.Term(name, word) for word in words], idx)\n        return builder.build()\n\n    @classmethod\n    def wildcard(cls, name: str, value) -&gt; 'Query':\n        \"\"\"Return lucene WildcardQuery.\"\"\"\n        return cls(search.WildcardQuery, index.Term(name, value))\n\n    @classmethod\n    def fuzzy(cls, name: str, value, *args) -&gt; 'Query':\n        \"\"\"Return lucene FuzzyQuery.\"\"\"\n        return cls(search.FuzzyQuery, index.Term(name, value), *args)\n\n    @classmethod\n    def alldocs(cls) -&gt; 'Query':\n        \"\"\"Return lucene MatchAllDocsQuery.\"\"\"\n        return cls(search.MatchAllDocsQuery)\n\n    @classmethod\n    def nodocs(cls) -&gt; 'Query':\n        \"\"\"Return lucene MatchNoDocsQuery.\"\"\"\n        return cls(search.MatchNoDocsQuery)\n\n    @classmethod\n    def regexp(cls, name: str, value, *args) -&gt; 'Query':\n        \"\"\"Return lucene RegexpQuery.\"\"\"\n        return cls(search.RegexpQuery, index.Term(name, value), *args)\n\n    @staticmethod\n    def points(name: str, *values) -&gt; search.Query:\n        \"\"\"Return lucene set query of one dimensional points.\"\"\"\n        if any(isinstance(value, float) for value in values):\n            return document.DoublePoint.newSetQuery(name, values)\n        return document.LongPoint.newSetQuery(name, tuple(map(int, values)))\n\n    @staticmethod\n    def ranges(name: str, *intervals, lower=True, upper=False) -&gt; search.Query:\n        \"\"\"Return lucene multidimensional point range query, by default with half-open intervals.\"\"\"\n        starts, stops = [], []\n        for start, stop in intervals:\n            if isinstance(start, float) or isinstance(stop, float):\n                if start is None:\n                    start = Double.NEGATIVE_INFINITY\n                elif not lower:\n                    start = document.DoublePoint.nextUp(start)\n                if stop is None:\n                    stop = Double.POSITIVE_INFINITY\n                elif not upper:\n                    stop = document.DoublePoint.nextDown(stop)\n            else:\n                if start is None:\n                    start = Long.MIN_VALUE\n                elif not lower:\n                    start += 1\n                if stop is None:\n                    stop = Long.MAX_VALUE\n                elif not upper:\n                    stop -= 1\n            starts.append(start)\n            stops.append(stop)\n        if any(isinstance(value, float) for value in starts):\n            return document.DoublePoint.newRangeQuery(name, starts, stops)\n        return document.LongPoint.newRangeQuery(name, starts, stops)\n\n    def constant(self) -&gt; 'Query':\n        \"\"\"Return lucene ConstantScoreQuery.\"\"\"\n        return Query(search.ConstantScoreQuery, self)\n\n    def boost(self, value: float) -&gt; 'Query':\n        \"\"\"Return lucene BoostQuery.\"\"\"\n        return Query(search.BoostQuery, self, value)\n\n    def __pos__(self) -&gt; search.BooleanQuery:\n        \"\"\"+self\"\"\"\n        return Query.all(self)\n\n    def __neg__(self) -&gt; search.BooleanQuery:\n        \"\"\"-self\"\"\"\n        return Query.boolean(search.BooleanClause.Occur.MUST_NOT, self)\n\n    def __and__(self, other: search.Query) -&gt; search.BooleanQuery:\n        \"\"\"+self +other\"\"\"\n        return Query.all(self, other)\n\n    def __rand__(self, other):\n        return Query.all(other, self)\n\n    def __or__(self, other: search.Query) -&gt; search.BooleanQuery:\n        \"\"\"self other\"\"\"\n        return Query.any(self, other)\n\n    def __ror__(self, other):\n        return Query.any(other, self)\n\n    def __sub__(self, other: search.Query) -&gt; search.BooleanQuery:\n        \"\"\"self -other\"\"\"\n        builder = search.BooleanQuery.Builder()\n        builder.add(self, search.BooleanClause.Occur.SHOULD)\n        builder.add(other, search.BooleanClause.Occur.MUST_NOT)\n        return builder.build()\n\n    def __rsub__(self, other):\n        return Query.__sub__(other, self)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.__and__","title":"<code>__and__(other)</code>","text":"<p>+self +other</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def __and__(self, other: search.Query) -&gt; search.BooleanQuery:\n    \"\"\"+self +other\"\"\"\n    return Query.all(self, other)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.__neg__","title":"<code>__neg__()</code>","text":"<p>-self</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def __neg__(self) -&gt; search.BooleanQuery:\n    \"\"\"-self\"\"\"\n    return Query.boolean(search.BooleanClause.Occur.MUST_NOT, self)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.__or__","title":"<code>__or__(other)</code>","text":"<p>self other</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def __or__(self, other: search.Query) -&gt; search.BooleanQuery:\n    \"\"\"self other\"\"\"\n    return Query.any(self, other)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.__pos__","title":"<code>__pos__()</code>","text":"<p>+self</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def __pos__(self) -&gt; search.BooleanQuery:\n    \"\"\"+self\"\"\"\n    return Query.all(self)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.__sub__","title":"<code>__sub__(other)</code>","text":"<p>self -other</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def __sub__(self, other: search.Query) -&gt; search.BooleanQuery:\n    \"\"\"self -other\"\"\"\n    builder = search.BooleanQuery.Builder()\n    builder.add(self, search.BooleanClause.Occur.SHOULD)\n    builder.add(other, search.BooleanClause.Occur.MUST_NOT)\n    return builder.build()\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.all","title":"<code>all(*queries, **terms)</code>  <code>classmethod</code>","text":"<p>Return lucene BooleanQuery with MUST clauses from queries and terms.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef all(cls, *queries: search.Query, **terms) -&gt; search.BooleanQuery:\n    \"\"\"Return lucene BooleanQuery with MUST clauses from queries and terms.\"\"\"\n    return cls.boolean(search.BooleanClause.Occur.MUST, *queries, **terms)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.alldocs","title":"<code>alldocs()</code>  <code>classmethod</code>","text":"<p>Return lucene MatchAllDocsQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef alldocs(cls) -&gt; 'Query':\n    \"\"\"Return lucene MatchAllDocsQuery.\"\"\"\n    return cls(search.MatchAllDocsQuery)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.any","title":"<code>any(*queries, **terms)</code>  <code>classmethod</code>","text":"<p>Return lucene BooleanQuery with SHOULD clauses from queries and terms.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef any(cls, *queries: search.Query, **terms) -&gt; search.BooleanQuery:\n    \"\"\"Return lucene BooleanQuery with SHOULD clauses from queries and terms.\"\"\"\n    return cls.boolean(search.BooleanClause.Occur.SHOULD, *queries, **terms)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.boost","title":"<code>boost(value)</code>","text":"<p>Return lucene BoostQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def boost(self, value: float) -&gt; 'Query':\n    \"\"\"Return lucene BoostQuery.\"\"\"\n    return Query(search.BoostQuery, self, value)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.constant","title":"<code>constant()</code>","text":"<p>Return lucene ConstantScoreQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def constant(self) -&gt; 'Query':\n    \"\"\"Return lucene ConstantScoreQuery.\"\"\"\n    return Query(search.ConstantScoreQuery, self)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.disjunct","title":"<code>disjunct(multiplier, *queries, **terms)</code>  <code>classmethod</code>","text":"<p>Return lucene DisjunctionMaxQuery from queries and terms.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef disjunct(cls, multiplier, *queries, **terms):\n    \"\"\"Return lucene DisjunctionMaxQuery from queries and terms.\"\"\"\n    queries = list(queries)\n    for name, values in terms.items():\n        if isinstance(values, str):\n            values = [values]\n        queries += (cls.term(name, value) for value in values)\n    return cls(search.DisjunctionMaxQuery, Arrays.asList(queries), multiplier)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.filter","title":"<code>filter(*queries, **terms)</code>  <code>classmethod</code>","text":"<p>Return lucene BooleanQuery with FILTER clauses from queries and terms.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef filter(cls, *queries: search.Query, **terms) -&gt; search.BooleanQuery:\n    \"\"\"Return lucene BooleanQuery with FILTER clauses from queries and terms.\"\"\"\n    return cls.boolean(search.BooleanClause.Occur.FILTER, *queries, **terms)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.fuzzy","title":"<code>fuzzy(name, value, *args)</code>  <code>classmethod</code>","text":"<p>Return lucene FuzzyQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef fuzzy(cls, name: str, value, *args) -&gt; 'Query':\n    \"\"\"Return lucene FuzzyQuery.\"\"\"\n    return cls(search.FuzzyQuery, index.Term(name, value), *args)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.near","title":"<code>near(name, *values, **kwargs)</code>  <code>classmethod</code>","text":"<p>Return SpanNearQuery from terms. Term values which supply another field name will be masked.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef near(cls, name: str, *values, **kwargs) -&gt; 'SpanQuery':\n    \"\"\"Return [SpanNearQuery][lupyne.engine.queries.SpanQuery.near] from terms.\n    Term values which supply another field name will be masked.\"\"\"\n    spans = (\n        cls.span(name, value) if isinstance(value, str) else cls.span(*value).mask(name)\n        for value in values\n    )\n    return SpanQuery.near(*spans, **kwargs)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.nodocs","title":"<code>nodocs()</code>  <code>classmethod</code>","text":"<p>Return lucene MatchNoDocsQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef nodocs(cls) -&gt; 'Query':\n    \"\"\"Return lucene MatchNoDocsQuery.\"\"\"\n    return cls(search.MatchNoDocsQuery)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.phrase","title":"<code>phrase(name, *values, **attrs)</code>  <code>classmethod</code>","text":"<p>Return lucene MultiPhraseQuery.  None may be used as a placeholder.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef phrase(cls, name: str, *values, **attrs) -&gt; search.MultiPhraseQuery:\n    \"\"\"Return lucene MultiPhraseQuery.  None may be used as a placeholder.\"\"\"\n    builder = search.MultiPhraseQuery.Builder()\n    for attr in attrs:\n        setattr(builder, attr, attrs[attr])\n    for idx, words in enumerate(values):\n        if isinstance(words, str):\n            words = [words]\n        if words is not None:\n            builder.add([index.Term(name, word) for word in words], idx)\n    return builder.build()\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.points","title":"<code>points(name, *values)</code>  <code>staticmethod</code>","text":"<p>Return lucene set query of one dimensional points.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@staticmethod\ndef points(name: str, *values) -&gt; search.Query:\n    \"\"\"Return lucene set query of one dimensional points.\"\"\"\n    if any(isinstance(value, float) for value in values):\n        return document.DoublePoint.newSetQuery(name, values)\n    return document.LongPoint.newSetQuery(name, tuple(map(int, values)))\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.prefix","title":"<code>prefix(name, value)</code>  <code>classmethod</code>","text":"<p>Return lucene PrefixQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef prefix(cls, name: str, value) -&gt; 'Query':\n    \"\"\"Return lucene PrefixQuery.\"\"\"\n    return cls(search.PrefixQuery, index.Term(name, value))\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.range","title":"<code>range(name, start, stop, lower=True, upper=False)</code>  <code>classmethod</code>","text":"<p>Return lucene RangeQuery, by default with a half-open interval.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef range(cls, name: str, start, stop, lower=True, upper=False) -&gt; 'Query':\n    \"\"\"Return lucene RangeQuery, by default with a half-open interval.\"\"\"\n    start, stop = (value if value is None else util.BytesRef(value) for value in (start, stop))\n    return cls(search.TermRangeQuery, name, start, stop, lower, upper)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.ranges","title":"<code>ranges(name, *intervals, lower=True, upper=False)</code>  <code>staticmethod</code>","text":"<p>Return lucene multidimensional point range query, by default with half-open intervals.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@staticmethod\ndef ranges(name: str, *intervals, lower=True, upper=False) -&gt; search.Query:\n    \"\"\"Return lucene multidimensional point range query, by default with half-open intervals.\"\"\"\n    starts, stops = [], []\n    for start, stop in intervals:\n        if isinstance(start, float) or isinstance(stop, float):\n            if start is None:\n                start = Double.NEGATIVE_INFINITY\n            elif not lower:\n                start = document.DoublePoint.nextUp(start)\n            if stop is None:\n                stop = Double.POSITIVE_INFINITY\n            elif not upper:\n                stop = document.DoublePoint.nextDown(stop)\n        else:\n            if start is None:\n                start = Long.MIN_VALUE\n            elif not lower:\n                start += 1\n            if stop is None:\n                stop = Long.MAX_VALUE\n            elif not upper:\n                stop -= 1\n        starts.append(start)\n        stops.append(stop)\n    if any(isinstance(value, float) for value in starts):\n        return document.DoublePoint.newRangeQuery(name, starts, stops)\n    return document.LongPoint.newRangeQuery(name, starts, stops)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.regexp","title":"<code>regexp(name, value, *args)</code>  <code>classmethod</code>","text":"<p>Return lucene RegexpQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef regexp(cls, name: str, value, *args) -&gt; 'Query':\n    \"\"\"Return lucene RegexpQuery.\"\"\"\n    return cls(search.RegexpQuery, index.Term(name, value), *args)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.span","title":"<code>span(*term)</code>  <code>classmethod</code>","text":"<p>Return SpanQuery from term name and value or a MultiTermQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef span(cls, *term) -&gt; 'SpanQuery':\n    \"\"\"Return [SpanQuery][lupyne.engine.queries.SpanQuery] from term name and value or a MultiTermQuery.\"\"\"\n    if len(term) &lt;= 1:\n        return SpanQuery(spans.SpanMultiTermQueryWrapper, *term)\n    return SpanQuery(spans.SpanTermQuery, index.Term(*term))\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.term","title":"<code>term(name, value)</code>  <code>classmethod</code>","text":"<p>Return lucene TermQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef term(cls, name: str, value) -&gt; 'Query':\n    \"\"\"Return lucene TermQuery.\"\"\"\n    return cls(search.TermQuery, index.Term(name, value))\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.terms","title":"<code>terms(name, values)</code>  <code>classmethod</code>","text":"<p>Return lucene TermInSetQuery, optimizing a SHOULD BooleanQuery of many terms.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef terms(cls, name: str, values) -&gt; 'Query':\n    \"\"\"Return lucene TermInSetQuery, optimizing a SHOULD BooleanQuery of many terms.\"\"\"\n    return cls(search.TermInSetQuery, name, Arrays.asList(list(map(util.BytesRef, values))))\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.Query.wildcard","title":"<code>wildcard(name, value)</code>  <code>classmethod</code>","text":"<p>Return lucene WildcardQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>@classmethod\ndef wildcard(cls, name: str, value) -&gt; 'Query':\n    \"\"\"Return lucene WildcardQuery.\"\"\"\n    return cls(search.WildcardQuery, index.Term(name, value))\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpanQuery","title":"<code>lupyne.engine.queries.SpanQuery</code>","text":"<p>               Bases: <code>Query</code></p> <p>Inherited lucene SpanQuery with additional span constructors.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>class SpanQuery(Query):\n    \"\"\"Inherited lucene SpanQuery with additional span constructors.\"\"\"\n\n    def __getitem__(self, slc: slice) -&gt; 'SpanQuery':\n        start, stop, step = slc.indices(Integer.MAX_VALUE)\n        assert step == 1, 'slice step is not supported'\n        return SpanQuery(spans.SpanPositionRangeQuery, self, start, stop)\n\n    def __sub__(self, other: spans.SpanQuery) -&gt; 'SpanQuery':\n        return SpanQuery(spans.SpanNotQuery, self, other)\n\n    def __or__(*spans_: spans.SpanQuery) -&gt; 'SpanQuery':\n        return SpanQuery(spans.SpanOrQuery, spans_)\n\n    def near(*spans_, slop=0, inOrder=True):\n        \"\"\"Return lucene SpanNearQuery from SpanQueries.\"\"\"\n        return SpanQuery(spans.SpanNearQuery, spans_, slop, inOrder)\n\n    def mask(self, name: str) -&gt; 'SpanQuery':\n        \"\"\"Return lucene FieldMaskingSpanQuery, which allows combining span queries from different fields.\"\"\"\n        return SpanQuery(spans.FieldMaskingSpanQuery, self, name)\n\n    def containing(self, other: spans.SpanQuery) -&gt; 'SpanQuery':\n        \"\"\"Return lucene SpanContainingQuery.\"\"\"\n        return SpanQuery(spans.SpanContainingQuery, self, other)\n\n    def within(self, other: spans.SpanQuery) -&gt; 'SpanQuery':\n        \"\"\"Return lucene SpanWithinQuery.\"\"\"\n        return SpanQuery(spans.SpanWithinQuery, self, other)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpanQuery.containing","title":"<code>containing(other)</code>","text":"<p>Return lucene SpanContainingQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def containing(self, other: spans.SpanQuery) -&gt; 'SpanQuery':\n    \"\"\"Return lucene SpanContainingQuery.\"\"\"\n    return SpanQuery(spans.SpanContainingQuery, self, other)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpanQuery.mask","title":"<code>mask(name)</code>","text":"<p>Return lucene FieldMaskingSpanQuery, which allows combining span queries from different fields.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def mask(self, name: str) -&gt; 'SpanQuery':\n    \"\"\"Return lucene FieldMaskingSpanQuery, which allows combining span queries from different fields.\"\"\"\n    return SpanQuery(spans.FieldMaskingSpanQuery, self, name)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpanQuery.near","title":"<code>near(*spans_, slop=0, inOrder=True)</code>","text":"<p>Return lucene SpanNearQuery from SpanQueries.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def near(*spans_, slop=0, inOrder=True):\n    \"\"\"Return lucene SpanNearQuery from SpanQueries.\"\"\"\n    return SpanQuery(spans.SpanNearQuery, spans_, slop, inOrder)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpanQuery.within","title":"<code>within(other)</code>","text":"<p>Return lucene SpanWithinQuery.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def within(self, other: spans.SpanQuery) -&gt; 'SpanQuery':\n    \"\"\"Return lucene SpanWithinQuery.\"\"\"\n    return SpanQuery(spans.SpanWithinQuery, self, other)\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpellParser","title":"<code>lupyne.engine.queries.SpellParser</code>","text":"<p>               Bases: <code>PythonQueryParser</code></p> <p>Inherited lucene QueryParser which corrects spelling.</p> <p>Assign a searcher attribute or override suggest implementation.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>class SpellParser(PythonQueryParser):\n    \"\"\"Inherited lucene QueryParser which corrects spelling.\n\n    Assign a searcher attribute or override [suggest][lupyne.engine.queries.SpellParser.suggest] implementation.\n    \"\"\"\n\n    def suggest(self, term: index.Term) -&gt; index.Term:\n        \"\"\"Return term with text replaced as necessary.\"\"\"\n        field = term.field()\n        words = self.searcher.suggest(field, term.text())\n        return index.Term(field, *words) if words else term\n\n    def rewrite(self, query: search.Query) -&gt; search.Query:\n        \"\"\"Return term or phrase query with corrected terms substituted.\"\"\"\n        if search.TermQuery.instance_(query):\n            term = search.TermQuery.cast_(query).term\n            return search.TermQuery(self.suggest(term))\n        query = search.PhraseQuery.cast_(query)\n        builder = search.PhraseQuery.Builder()\n        for position, term in zip(query.positions, query.terms):\n            builder.add(self.suggest(term), position)\n        return builder.build()\n\n    def getFieldQuery_quoted(self, *args):\n        return self.rewrite(self.getFieldQuery_quoted_super(*args))\n\n    def getFieldQuery_slop(self, *args):\n        return self.rewrite(self.getFieldQuery_slop_super(*args))\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpellParser.rewrite","title":"<code>rewrite(query)</code>","text":"<p>Return term or phrase query with corrected terms substituted.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def rewrite(self, query: search.Query) -&gt; search.Query:\n    \"\"\"Return term or phrase query with corrected terms substituted.\"\"\"\n    if search.TermQuery.instance_(query):\n        term = search.TermQuery.cast_(query).term\n        return search.TermQuery(self.suggest(term))\n    query = search.PhraseQuery.cast_(query)\n    builder = search.PhraseQuery.Builder()\n    for position, term in zip(query.positions, query.terms):\n        builder.add(self.suggest(term), position)\n    return builder.build()\n</code></pre>"},{"location":"engine/#lupyne.engine.queries.SpellParser.suggest","title":"<code>suggest(term)</code>","text":"<p>Return term with text replaced as necessary.</p> Source code in <code>lupyne/engine/queries.py</code> <pre><code>def suggest(self, term: index.Term) -&gt; index.Term:\n    \"\"\"Return term with text replaced as necessary.\"\"\"\n    field = term.field()\n    words = self.searcher.suggest(field, term.text())\n    return index.Term(field, *words) if words else term\n</code></pre>"},{"location":"examples/","title":"Examples","text":"In\u00a0[1]: Copied! <pre>import shutil\nimport lucene\nfrom java.io import File\nfrom org.apache.lucene import analysis, document, index, queryparser, search, store\nfrom lupyne import engine\n\nassert lucene.getVMEnv() or lucene.initVM()\n\nanalyzer = analysis.standard.StandardAnalyzer()\n\ndirectory = store.FSDirectory.open(File('tempIndex').toPath())\nconfig = index.IndexWriterConfig(analyzer)\niwriter = index.IndexWriter(directory, config)\ndoc = document.Document()\ntext = \"This is the text to be indexed.\"\ndoc.add(document.Field('fieldname', text, document.TextField.TYPE_STORED))\niwriter.addDocument(doc)\niwriter.close()\n\n# Now search the index:\nireader = index.DirectoryReader.open(directory)\nisearcher = search.IndexSearcher(ireader)\n# Parse a simple query that searches for \"text\":\nparser = queryparser.classic.QueryParser('fieldname', analyzer)\nquery = parser.parse('text')\nhits = isearcher.search(query, 10).scoreDocs\nassert len(hits) == 1\n# Iterate through the results:\nstoredFields = isearcher.storedFields()\nfor hit in hits:\n    hitDoc = storedFields.document(hit.doc)\n    assert hitDoc['fieldname'] == text\nireader.close()\ndirectory.close()\nshutil.rmtree('tempIndex')\n</pre> import shutil import lucene from java.io import File from org.apache.lucene import analysis, document, index, queryparser, search, store from lupyne import engine  assert lucene.getVMEnv() or lucene.initVM()  analyzer = analysis.standard.StandardAnalyzer()  directory = store.FSDirectory.open(File('tempIndex').toPath()) config = index.IndexWriterConfig(analyzer) iwriter = index.IndexWriter(directory, config) doc = document.Document() text = \"This is the text to be indexed.\" doc.add(document.Field('fieldname', text, document.TextField.TYPE_STORED)) iwriter.addDocument(doc) iwriter.close()  # Now search the index: ireader = index.DirectoryReader.open(directory) isearcher = search.IndexSearcher(ireader) # Parse a simple query that searches for \"text\": parser = queryparser.classic.QueryParser('fieldname', analyzer) query = parser.parse('text') hits = isearcher.search(query, 10).scoreDocs assert len(hits) == 1 # Iterate through the results: storedFields = isearcher.storedFields() for hit in hits:     hitDoc = storedFields.document(hit.doc)     assert hitDoc['fieldname'] == text ireader.close() directory.close() shutil.rmtree('tempIndex') <pre>WARNING: Using incubator modules: jdk.incubator.vector\nNov 02, 2025 11:16:26 PM org.apache.lucene.internal.vectorization.PanamaVectorizationProvider &lt;init&gt;\nINFO: Java vector incubator API enabled; uses preferredBitSize=256; FMA enabled\n</pre> In\u00a0[2]: Copied! <pre># Indexer combines Writer and Searcher; StandardAnalyzer is the default\nindexer = engine.Indexer('tempIndex')\n# default indexed text settings for documents\nindexer.set('fieldname', engine.Field.Text, stored=True)\nindexer.add(fieldname=text)  # add document\nindexer.commit()  # commit changes and refresh searcher\n\nhits = indexer.search('text', field='fieldname')  # parsing handled if necessary\nassert len(hits) == 1\nfor hit in hits:  # hits support mapping interface\n    assert hit['fieldname'] == text\n# closing is handled automatically\ndel indexer\nshutil.rmtree('tempIndex')\n</pre> # Indexer combines Writer and Searcher; StandardAnalyzer is the default indexer = engine.Indexer('tempIndex') # default indexed text settings for documents indexer.set('fieldname', engine.Field.Text, stored=True) indexer.add(fieldname=text)  # add document indexer.commit()  # commit changes and refresh searcher  hits = indexer.search('text', field='fieldname')  # parsing handled if necessary assert len(hits) == 1 for hit in hits:  # hits support mapping interface     assert hit['fieldname'] == text # closing is handled automatically del indexer shutil.rmtree('tempIndex') In\u00a0[3]: Copied! <pre>from org.apache.lucene.queries import spans\n\nq1 = search.TermQuery(index.Term('text', 'lucene'))\nq2 = (\n    search.PhraseQuery.Builder()\n    .add(index.Term('text', 'search'))\n    .add(index.Term('text', 'engine'))\n    .build()\n)\nsearch.BooleanQuery.Builder().add(q1, search.BooleanClause.Occur.MUST).add(\n    q2, search.BooleanClause.Occur.MUST\n).build()\n</pre> from org.apache.lucene.queries import spans  q1 = search.TermQuery(index.Term('text', 'lucene')) q2 = (     search.PhraseQuery.Builder()     .add(index.Term('text', 'search'))     .add(index.Term('text', 'engine'))     .build() ) search.BooleanQuery.Builder().add(q1, search.BooleanClause.Occur.MUST).add(     q2, search.BooleanClause.Occur.MUST ).build() Out[3]: <pre>&lt;BooleanQuery: +text:lucene +text:\"search engine\"&gt;</pre> In\u00a0[4]: Copied! <pre>q1 = spans.SpanTermQuery(index.Term('text', 'hello'))\nq2 = spans.SpanTermQuery(index.Term('text', 'world'))\nq3 = spans.SpanPositionRangeQuery(q1, 0, 10)\nq4 = spans.SpanNearQuery([q1, q2], 0, True)\nspans.SpanNotQuery(q3, q4)\n</pre> q1 = spans.SpanTermQuery(index.Term('text', 'hello')) q2 = spans.SpanTermQuery(index.Term('text', 'world')) q3 = spans.SpanPositionRangeQuery(q1, 0, 10) q4 = spans.SpanNearQuery([q1, q2], 0, True) spans.SpanNotQuery(q3, q4) Out[4]: <pre>&lt;SpanNotQuery: spanNot(spanPosRange(text:hello, 0, 10), spanNear([text:hello, text:world], 0, true), 0, 0)&gt;</pre> In\u00a0[5]: Copied! <pre>Q = engine.Query\n\nQ.term('text', 'lucene') &amp; Q.phrase('text', 'search', 'engine')\n</pre> Q = engine.Query  Q.term('text', 'lucene') &amp; Q.phrase('text', 'search', 'engine') Out[5]: <pre>&lt;BooleanQuery: +text:lucene +text:\"search engine\"&gt;</pre> In\u00a0[6]: Copied! <pre>Q.span('text', 'hello')[:10] - Q.near('text', 'hello', 'world')\n</pre> Q.span('text', 'hello')[:10] - Q.near('text', 'hello', 'world') Out[6]: <pre>&lt;SpanNotQuery: spanNot(spanPosRange(text:hello, 0, 10), spanNear([text:hello, text:world], 0, true), 0, 0)&gt;</pre> In\u00a0[7]: Copied! <pre>from datetime import date\nfrom org.apache.lucene import geo\n\ndocs = [\n    {\n        'city': 'San Francisco',\n        'state': 'CA',\n        'incorporated': '1850-04-15',\n        'population': 808976,\n        'longitude': -122.4192,\n        'latitude': 37.7752,\n    },\n    {\n        'city': 'Los Angeles',\n        'state': 'CA',\n        'incorporated': '1850-04-04',\n        'population': 3849378,\n        'longitude': -118.2434,\n        'latitude': 34.0521,\n    },\n    {\n        'city': 'Portland',\n        'state': 'OR',\n        'incorporated': '1851-02-08',\n        'population': 575930,\n        'longitude': -122.6703,\n        'latitude': 45.5238,\n    },\n]\n\nindexer = engine.Indexer('tempIndex')\nindexer.set('city', stored=True)\nindexer.set('state', stored=True)\n# set method supports custom field types inheriting their default settings\nindexer.set('incorporated', engine.DateTimeField)\nindexer.set('year-month-day', engine.NestedField, sep='-')\nindexer.set('population', dimensions=1)\nindexer.set('point', engine.ShapeField)\n# assigned fields can have a different key from their underlying field name\nindexer.fields['location'] = engine.NestedField('state.city')\n\nfor doc in docs:\n    doc['year-month-day'] = doc['incorporated']\n    point = geo.Point(doc.pop('latitude'), doc.pop('longitude'))\n    location = doc['state'] + '.' + doc['city']\n    incorporated = map(int, doc.pop('incorporated').split('-'))\n    indexer.add(doc, location=location, incorporated=date(*incorporated), point=point)\nindexer.commit()\n\nquery = indexer.fields['incorporated'].prefix([1850])\n[hit['city'] for hit in indexer.search(query)]\n</pre> from datetime import date from org.apache.lucene import geo  docs = [     {         'city': 'San Francisco',         'state': 'CA',         'incorporated': '1850-04-15',         'population': 808976,         'longitude': -122.4192,         'latitude': 37.7752,     },     {         'city': 'Los Angeles',         'state': 'CA',         'incorporated': '1850-04-04',         'population': 3849378,         'longitude': -118.2434,         'latitude': 34.0521,     },     {         'city': 'Portland',         'state': 'OR',         'incorporated': '1851-02-08',         'population': 575930,         'longitude': -122.6703,         'latitude': 45.5238,     }, ]  indexer = engine.Indexer('tempIndex') indexer.set('city', stored=True) indexer.set('state', stored=True) # set method supports custom field types inheriting their default settings indexer.set('incorporated', engine.DateTimeField) indexer.set('year-month-day', engine.NestedField, sep='-') indexer.set('population', dimensions=1) indexer.set('point', engine.ShapeField) # assigned fields can have a different key from their underlying field name indexer.fields['location'] = engine.NestedField('state.city')  for doc in docs:     doc['year-month-day'] = doc['incorporated']     point = geo.Point(doc.pop('latitude'), doc.pop('longitude'))     location = doc['state'] + '.' + doc['city']     incorporated = map(int, doc.pop('incorporated').split('-'))     indexer.add(doc, location=location, incorporated=date(*incorporated), point=point) indexer.commit()  query = indexer.fields['incorporated'].prefix([1850]) [hit['city'] for hit in indexer.search(query)] Out[7]: <pre>['San Francisco', 'Los Angeles']</pre> In\u00a0[8]: Copied! <pre>query = indexer.fields['incorporated'].range(date(1850, 4, 10), None)\n[hit['city'] for hit in indexer.search(query)]\n</pre> query = indexer.fields['incorporated'].range(date(1850, 4, 10), None) [hit['city'] for hit in indexer.search(query)] Out[8]: <pre>['San Francisco', 'Portland']</pre> In\u00a0[9]: Copied! <pre>query = indexer.fields['year-month-day'].prefix('1850')\nquery\n</pre> query = indexer.fields['year-month-day'].prefix('1850') query Out[9]: <pre>&lt;PrefixQuery: year:1850*&gt;</pre> In\u00a0[10]: Copied! <pre>[hit['city'] for hit in indexer.search(query)]\n</pre> [hit['city'] for hit in indexer.search(query)] Out[10]: <pre>['San Francisco', 'Los Angeles']</pre> In\u00a0[11]: Copied! <pre>query = indexer.fields['year-month-day'].range('1850-04-10', None)\nquery\n</pre> query = indexer.fields['year-month-day'].range('1850-04-10', None) query Out[11]: <pre>&lt;TermRangeQuery: year-month-day:[1850-04-10 TO *}&gt;</pre> In\u00a0[12]: Copied! <pre>[hit['city'] for hit in indexer.search(query)]\n</pre> [hit['city'] for hit in indexer.search(query)] Out[12]: <pre>['San Francisco', 'Portland']</pre> In\u00a0[13]: Copied! <pre>query = Q.ranges('population', (0, 1000000))\n[hit['city'] for hit in indexer.search(query)]\n</pre> query = Q.ranges('population', (0, 1000000)) [hit['city'] for hit in indexer.search(query)] Out[13]: <pre>['San Francisco', 'Portland']</pre> In\u00a0[14]: Copied! <pre>cities = ['San Francisco', 'Los Angeles', 'Portland']\nfor distance in [1e3, 1e5, 7e5, 1e6]:\n    query = indexer.fields['point'].within(geo.Circle(37.7, -122.4, distance))\n    print([hit['city'] for hit in indexer.search(query)])\n</pre> cities = ['San Francisco', 'Los Angeles', 'Portland'] for distance in [1e3, 1e5, 7e5, 1e6]:     query = indexer.fields['point'].within(geo.Circle(37.7, -122.4, distance))     print([hit['city'] for hit in indexer.search(query)]) <pre>[]\n['San Francisco']\n['San Francisco', 'Los Angeles']\n['San Francisco', 'Los Angeles', 'Portland']\n</pre> In\u00a0[15]: Copied! <pre>query = indexer.fields['location'].prefix('CA.San')\nquery  # works like any prefix query\n</pre> query = indexer.fields['location'].prefix('CA.San') query  # works like any prefix query Out[15]: <pre>&lt;PrefixQuery: state.city:CA.San*&gt;</pre> In\u00a0[16]: Copied! <pre>[hit['city'] for hit in indexer.search(query)]\n</pre> [hit['city'] for hit in indexer.search(query)] Out[16]: <pre>['San Francisco']</pre> In\u00a0[17]: Copied! <pre>query = indexer.fields['location'].prefix('CA')\nquery  # optimized to search the best field\n</pre> query = indexer.fields['location'].prefix('CA') query  # optimized to search the best field Out[17]: <pre>&lt;PrefixQuery: state:CA*&gt;</pre> In\u00a0[18]: Copied! <pre>[hit['city'] for hit in indexer.search(query)]\ndel indexer\nshutil.rmtree('tempIndex')\n</pre> [hit['city'] for hit in indexer.search(query)] del indexer shutil.rmtree('tempIndex') In\u00a0[19]: Copied! <pre>colors = 'red', 'green', 'blue', 'cyan', 'magenta', 'yellow'\nindexer = engine.Indexer('tempIndex')\nindexer.set('color', engine.Field.String, stored=True, docValuesType='sorted')\nfor color in colors:\n    indexer.add(color=color)\nindexer.commit()\n\nsearcher = search.IndexSearcher(indexer.indexReader)\nsorter = search.Sort(search.SortField('color', search.SortField.Type.STRING))\ntopdocs = searcher.search(search.MatchAllDocsQuery(), 10, sorter)\nstoredFields = searcher.storedFields()\n[storedFields.document(scoredoc.doc)['color'] for scoredoc in topdocs.scoreDocs]\n</pre> colors = 'red', 'green', 'blue', 'cyan', 'magenta', 'yellow' indexer = engine.Indexer('tempIndex') indexer.set('color', engine.Field.String, stored=True, docValuesType='sorted') for color in colors:     indexer.add(color=color) indexer.commit()  searcher = search.IndexSearcher(indexer.indexReader) sorter = search.Sort(search.SortField('color', search.SortField.Type.STRING)) topdocs = searcher.search(search.MatchAllDocsQuery(), 10, sorter) storedFields = searcher.storedFields() [storedFields.document(scoredoc.doc)['color'] for scoredoc in topdocs.scoreDocs] Out[19]: <pre>['blue', 'cyan', 'green', 'magenta', 'red', 'yellow']</pre> In\u00a0[20]: Copied! <pre>hits = indexer.search(sort='color')\n[hit['color'] for hit in hits]\n</pre> hits = indexer.search(sort='color') [hit['color'] for hit in hits] Out[20]: <pre>['blue', 'cyan', 'green', 'magenta', 'red', 'yellow']</pre> In\u00a0[21]: Copied! <pre>docvalues = hits.docvalues('color')\ndocvalues\n</pre> docvalues = hits.docvalues('color') docvalues Out[21]: <pre>{0: 'red', 1: 'green', 2: 'blue', 3: 'cyan', 4: 'magenta', 5: 'yellow'}</pre> In\u00a0[22]: Copied! <pre>hits = indexer.search().sorted(docvalues.__getitem__)\n[hit['color'] for hit in hits]\ndel indexer\nshutil.rmtree('tempIndex')\n</pre> hits = indexer.search().sorted(docvalues.__getitem__) [hit['color'] for hit in hits] del indexer shutil.rmtree('tempIndex') In\u00a0[23]: Copied! <pre>import itertools\n\ncolors = 'red', 'green', 'blue', 'cyan', 'magenta', 'yellow'\nfacets = dict(zip(colors, itertools.count(1)))\nindexer = engine.Indexer('tempIndex')\nindexer.set('color', engine.Field.String, stored=True, docValuesType='sorted')\nfor color in facets:\n    for _ in range(facets[color]):\n        indexer.add(color=color)\nindexer.commit()\nquery = Q.alldocs()\n</pre> import itertools  colors = 'red', 'green', 'blue', 'cyan', 'magenta', 'yellow' facets = dict(zip(colors, itertools.count(1))) indexer = engine.Indexer('tempIndex') indexer.set('color', engine.Field.String, stored=True, docValuesType='sorted') for color in facets:     for _ in range(facets[color]):         indexer.add(color=color) indexer.commit() query = Q.alldocs() <p>Groupby using GroupingSearch.</p> In\u00a0[24]: Copied! <pre>for hits in indexer.groupby('color', query):\n    assert facets[hits.value] == hits.count\n    (hit,) = hits\n    assert hit['color'] == hits.value\n</pre> for hits in indexer.groupby('color', query):     assert facets[hits.value] == hits.count     (hit,) = hits     assert hit['color'] == hits.value <p>Groupby using Hits.</p> In\u00a0[25]: Copied! <pre>hits = indexer.search(query)\nfor hits in hits.groupby(hits.docvalues('color').__getitem__, docs=1):\n    assert facets[hits.value] == hits.count\n    (hit,) = hits\n    assert hit['color'] == hits.value\n</pre> hits = indexer.search(query) for hits in hits.groupby(hits.docvalues('color').__getitem__, docs=1):     assert facets[hits.value] == hits.count     (hit,) = hits     assert hit['color'] == hits.value <p>Facets using GroupingSearch.</p> In\u00a0[26]: Copied! <pre>indexer.facets(query, 'color')\n</pre> indexer.facets(query, 'color') Out[26]: <pre>{'color': {'red': 1.0,\n  'green': 2.0,\n  'blue': 3.0,\n  'cyan': 4.0,\n  'magenta': 5.0,\n  'yellow': 6.0}}</pre> <p>Facets using query counts.</p> In\u00a0[27]: Copied! <pre>queries = {'additive': Q.any(color=colors[:3]), 'subtractive': Q.any(color=colors[3:])}\nindexer.facets(query, color=queries)\ndel indexer\nshutil.rmtree('tempIndex')\n</pre> queries = {'additive': Q.any(color=colors[:3]), 'subtractive': Q.any(color=colors[3:])} indexer.facets(query, color=queries) del indexer shutil.rmtree('tempIndex')"},{"location":"examples/#indexers","title":"indexers\u00b6","text":"<p>Basic indexing and searching adapted from lucene's documentation.</p>"},{"location":"examples/#lucene","title":"lucene\u00b6","text":""},{"location":"examples/#lupyne","title":"lupyne\u00b6","text":""},{"location":"examples/#queries","title":"queries\u00b6","text":"<p>Classmethods for convenient query building. Operator overloading is used for combining boolean clauses, provided at least one of the queries is wrapped by lupyne.</p>"},{"location":"examples/#lucene","title":"lucene\u00b6","text":""},{"location":"examples/#lupyne","title":"lupyne\u00b6","text":""},{"location":"examples/#searching","title":"searching\u00b6","text":"<p>Advanced searching with custom fields.</p> <p>Lupyne ShapeFields and DateTimeFields are implemented as lucene Shape and Point fields. NestedFields simulate a composite index. The fields have convenience methods for creating prefix and range queries.</p>"},{"location":"examples/#sorting","title":"sorting\u00b6","text":"<p>PyLucene has several pitfalls when collecting or sorting a large query result. Generally they involve the overhead of traversing the VM in an internal loop.</p> <p>Lucene also requires supplying a maximum doc count for searches, and supplying an excessively large count is a poor workaround because the collection heap is pre-allocated.</p> <p>To mitigate these problems, Lupyne first provides a unified search interface. The same Hits type is returned regardless of optional doc count or sorting parameters. As with lucene, the result is fully evaluated but each individual Hit object will only be loaded on demand. Internally a CachingCollector is used when all docs are requested.</p> <p>The search method allows lucene Sort parameters to be passed through, since that's still optimal. Additionally the hits themselves can be sorted afterwards with any python callable key. The IndexReader.docvalues method is convenient for creating a sort key table from fields with docvalues. The upshot is custom sorting and sorting large results are both easier and faster.</p> <p>Custom sorting isn't necessary in the below example of course, just there for demonstration.</p>"},{"location":"examples/#lucene","title":"lucene\u00b6","text":""},{"location":"examples/#lupyne","title":"lupyne\u00b6","text":""},{"location":"examples/#grouping","title":"grouping\u00b6","text":"<p>Lupyne supports lucene's contrib grouping.GroupingSearch interface, but it has some limitations. GroupingSearch objects only support single-valued strings, and won't find zero-valued facets. Lupyne also supports grouping hits by an arbitrary function after the original search. Similar to sorting, the native approach is generally more efficient, proportional to the number of documents culled.</p> <p>Lupyne can also compute facet counts with intersected queries. Although seemingly less efficient, it may be faster with small numbers of terms. It also has no limitations on multiple values, and can be fully customized without reindexing.</p>"},{"location":"services/","title":"Services","text":"<p>Services use Starlette's config: in environment variables or a .env file</p>"},{"location":"services/#graphql","title":"GraphQL","text":"<pre><code>DIRECTORIES=... SCHEMA=... uvicorn lupyne.services.graphql:app\n</code></pre> <p>Open http://localhost:8000/graphql.</p> <p>Creating a graphql schema is strongly recommended, to retrieve stored fields and sort by fields.</p> <pre><code>\"\"\"stored fields\"\"\"\ntype Document {\n  name: String\n  tag: [String!]!\n  size: Int\n}\n\n\"\"\"sort fields\"\"\"\ntype FieldDoc {\n  date: String\n}\n</code></pre>"},{"location":"services/#rest","title":"REST","text":"<pre><code>DIRECTORIES=... SCHEMA=... uvicorn lupyne.services.rest:app\n</code></pre> <p>Open http://localhost:8000/docs.</p> <p>Creating a graphql schema is also recommended - even though it's REST - to retrieve stored fields and sort by fields.</p>"}]}